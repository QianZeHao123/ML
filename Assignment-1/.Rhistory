stat_smooth(method=lm,formula = y ~ x)+
theme_bw()
autowithcylgroups2 = autompgclean %>%
mutate(cyl_group = cut(cylinders, c(3,6,8), include.lowest=TRUE))
head(autowithcylgroups2)
summary(lm(mpg ~ horsepower*factor(cyl_group), data = autowithcylgroups2))
ggplot(autowithcylgroups2, aes(x=horsepower, y=mpg, group = cyl_group))+
geom_point(aes(colour=factor(cyl_group)))+
stat_smooth(method=lm,formula = y ~ x)+
theme_bw()
autowithgroups = autompgclean %>%
mutate(sub_group = cut(horsepower, c(46,100,180,230), include.lowest=TRUE))
ggplot(autowithgroups, aes(x=horsepower, y=mpg, group = sub_group))+
geom_point()+
stat_smooth(method=lm,formula = y ~ x)+
theme_bw()
summary(lm(mpg ~ horsepower*factor(sub_group), data = autowithgroups))
autowithgroups = autompgclean %>%
mutate(sub_group = cut(horsepower, c(46,100,150,230), include.lowest=TRUE))
ggplot(autowithgroups, aes(x=horsepower, y=mpg, group = sub_group))+
geom_point()+
stat_smooth(method=lm,formula = y ~ x)+
theme_bw()
summary(lm(mpg ~ horsepower*factor(sub_group), data = autowithgroups))
ggplot(autompgclean, aes(x=horsepower, y=mpg))+
geom_point()+
stat_smooth(method=lm,formula = y ~ poly(x,2))+
theme_bw()
summary(lm(mpg ~ poly(horsepower, 2), data = autompgclean))
autowithgroups = autompgclean %>%
mutate(sub_group = cut(cylinders, c(3,5,6,8), include.lowest=TRUE))
ggplot(autowithgroups, aes(x=horsepower, y=mpg, group = sub_group))+
geom_point()+
stat_smooth(method=lm,formula = y ~ poly(x,2))+
theme_bw()
summary(lm(mpg ~ poly(horsepower,2)*sub_group, data = autowithgroups))
auto_model1 = autompgclean %>%
mutate(sub_group = cut(cylinders, c(3,6,8), include.lowest=TRUE))
ggplot(auto_model1, aes(x=horsepower, y=mpg, group = sub_group))+
geom_point(aes(colour=factor(sub_group)))+
stat_smooth(method=lm,formula = y ~ poly(x,2))+
theme_bw()
auto_model2 = autompgclean %>%
mutate(sub_group = cut(horsepower, c(46,100,120,150,180,230), include.lowest=TRUE))
ggplot(auto_model2, aes(x=horsepower, y=mpg, group = sub_group))+
geom_point(aes(colour=factor(sub_group)))+
stat_smooth(method=lm,formula = y ~ poly(x,2))+
theme_bw()
library(splines)
knots = c(100, 150)
sp_model = lm(mpg ~ ns(horsepower, knots = knots), data=autompgclean)
summary(sp_model) #ignore all information about coefficients, they aren't meant to be interpreted in a traditional way.
ggplot(autompgclean, aes(x=horsepower, y=mpg))+
geom_point()+
stat_smooth(method=lm,formula = y ~ splines::ns(x, knots = knots))+
theme_bw()
ggplot(autompgclean, aes(x=horsepower, y=mpg))+
geom_point()+
stat_smooth(method=lm,formula = y ~ splines::ns(x, df = 3))+
theme_bw()
quant_model = lm(mpg ~ ns(horsepower, df = 3), data=autompgclean)
summary(quant_model)
library(caret)
ctrl_kfold = trainControl(method = "cv", number = 10)
lm_kfold = train(mpg ~ horsepower,
data = autompgclean,
method = "lm",
metric = "RMSE",
trControl = ctrl_kfold)
lm_kfold
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
data(mtcars)
pairs(mtcars,main = "Mtcars data", col='blue')
library("ggplot2")
library("GGally")
ggpairs(mtcars)+theme_bw()
# theme_bw is one of many themes you can modify from ggplot
# see https://ggplot2.tidyverse.org/reference/ggtheme.html for other themes
ols_model = lm(hp ~ mpg + drat + wt, data=mtcars)
summary(ols_model)
library(glmnet)
ridge0=glmnet(mtcars[,c("mpg","drat","wt")],mtcars$hp,
alpha=0,
lambda=0)
coef(ridge0)
lambdas = seq(0,10,by=0.5)
length(lambdas)
lambdas
ridge1=glmnet(mtcars[,c("mpg","drat","wt")],mtcars$hp,
alpha=0,
lambda=lambdas)
print(ridge1)
coef(ridge1,s = 0)
ridge1=glmnet(mtcars[,c("mpg","drat","wt")],mtcars$hp,
alpha=0,
lambda=lambdas,
thresh=1e-10)
coef(ridge1,s = 0)
lambda_new = seq(0,20,by=0.1)
ridge_test=glmnet(mtcars[,c("mpg","drat","wt")],mtcars$hp,
alpha=0,
lambda=lambda_new,
thresh=1e-10)
# coef(ridge_test, s = 60)
lambdas=10^seq(-3,3,by=0.1)
cv_fit = cv.glmnet(as.matrix(mtcars[,c("mpg","drat","wt")]),
as.matrix(mtcars$hp),
alpha=0,
lambda=lambdas)
plot(cv_fit)
cv_fit$lambda.min
coef(cv_fit, s="lambda.min")
predict_best_cv = predict(cv_fit, s="lambda.min",newx=as.matrix(mtcars[,c("mpg","drat","wt")]))
RMSE_best_cv = sqrt(mean((predict_best_cv-mtcars$hp)^2))
print(RMSE_best_cv)
lambdas=10^seq(-3,3,by=0.1)
cv_fit = cv.glmnet(as.matrix(mtcars[,c("mpg","drat","wt")]),
as.matrix(mtcars$hp),
alpha=0,
lambda=lambdas,
nfolds=20)
plot(cv_fit)
cv_fit$lambda.min
coef(cv_fit, s="lambda.min")
predict_best_cv = predict(cv_fit, s="lambda.min",newx=as.matrix(mtcars[,c("mpg","drat","wt")]))
RMSE_best_cv = sqrt(mean((predict_best_cv-mtcars$hp)^2))
print(RMSE_best_cv)
cv_fit$lambda.min
library(caret)
ctrl_kfold0 = trainControl(method = "cv", number = 10)
ridge_kfold0 = train(hp ~ mpg+drat+wt,
data = mtcars,
method = "glmnet",
metric = "RMSE",
thresh=1e-10,
tuneGrid = expand.grid(alpha = 0, lambda = 0),
trControl = ctrl_kfold0)
coef(ridge_kfold0$finalModel, ridge_kfold0$bestTune$lambda)
lambdas=10^seq(-3,3,by=0.1)
ctrl_kfold = trainControl(method = "cv", number = 10)
ridge_kfold = train(hp ~ mpg+drat+wt,
data = mtcars,
method = "glmnet",
metric = "RMSE",
tuneGrid = expand.grid(alpha = 0,
lambda = lambdas),
trControl = ctrl_kfold,
thresh=1e-10)
coef(ridge_kfold$finalModel, ridge_kfold$bestTune$lambda)
print(ridge_kfold$finalModel$lambdaOpt)
prediction_ridge = predict(ridge_kfold,mtcars[,c("mpg","drat","wt")])
RMSE(prediction_ridge,mtcars$hp)
set.seed(111)
# lm method
print('------------------lm method------------------')
lm_model = lm(hp ~ mpg + drat + wt, data=mtcars)
coef(lm_model)
# glmnet
set.seed(111)
print('------------------GLMnet method------------------')
lambdas=10^seq(-3,3,by=0.1)
cv_fit = cv.glmnet(as.matrix(mtcars[,c("mpg","drat","wt")]),
as.matrix(mtcars$hp),
alpha=0,
lambda=lambdas,
nfolds=10,
thresh=1e-10)
cv_fit$lambda.min
coef(cv_fit, s="lambda.min")
predict_best_cv = predict(cv_fit, s="lambda.min",newx=as.matrix(mtcars[,c("mpg","drat","wt")]))
RMSE_best_cv = sqrt(mean((predict_best_cv-mtcars$hp)^2))
print(RMSE_best_cv)
# CARet method
set.seed(111)
print('------------------CARet method------------------')
# lambdas=10^seq(-3,3,by=0.1)
ctrl_kfold = trainControl(method = "cv", number = 10)
ridge_kfold = train(hp ~ mpg+drat+wt,
data = mtcars,
method = "glmnet",
metric = "RMSE",
tuneGrid = expand.grid(alpha = 0,
lambda = lambdas),
trControl = ctrl_kfold,
nfolds=10,
thresh=1e-10)
print(ridge_kfold$finalModel$lambdaOpt)
coef(ridge_kfold$finalModel, ridge_kfold$bestTune$lambda)
prediction_ridge = predict(ridge_kfold,mtcars[,c("mpg","drat","wt")])
RMSE(prediction_ridge,mtcars$hp)
plot(ridge_kfold)
plot(cv_fit)
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
# current_directory
current_directory = getwd()
# read_csv
# joint file path
file = file.path(current_directory, "kc_house_data.csv")
HouseData = read.csv(file, header=TRUE)
HouseData =  subset(HouseData, select = c(id, price, bedrooms, bathrooms,
sqft_living, sqft_lot, floors,
yr_built, zipcode, lat, long))
dim(HouseData) #this should return 21613, 11
names(HouseData) #this should return id, price, bedrooms, bathrooms, sqft_living, sqft_lot, floors, yr_built, zipcode, lat, lon
head(HouseData)
library(leaps)
best_forward = regsubsets(price ~ ., data = HouseData, nbest = 1, method = "forward", nvmax = 10)
#print the summary for the best_forward output
summary(best_forward)
#save the summary to forward_summary
forward_summary = summary(best_forward)
#choose the model with the maximum adjusted R2
forward_adjr2 = which.max(forward_summary$adjr2)
#print the coefficients for the model with the highest adj R2
print(coef(best_forward,forward_adjr2))
forward_cp = which.min(forward_summary$cp)
print(coef(best_forward,forward_cp))
forward_bic = which.min(forward_summary$bic)
print(coef(best_forward,forward_bic))
par(mfrow = c(1,3)) # allows for 3 plots to be plotted side by side
plot(forward_summary$cp,
xlab = "# Predictors", #x-axis label
ylab = "Cp", #y-axis label
type = "l", #line plot
lwd = 2) #line thickness
cp_min = which.min(forward_summary$cp)
#overlay the minimum of cp on the previous plot using the points function
points(cp_min,
forward_summary$cp[cp_min],
pch = 4, #cross symbol used
col = 2, #red colour
cex = 4, #make it bigger!
lwd = 2) #make the cross lines thicker
# ADD CODE HERE TO PLOT BIC AND ADJUSTED R^2
plot(forward_summary$bic,
xlab = "# Predictors", #x-axis label
ylab = "BIC", #y-axis label
type = "l", #line plot
lwd = 2) #line thickness
bic_min = which.min(forward_summary$bic)
points(bic_min,
forward_summary$bic[bic_min],
pch = 4, #cross symbol used
col = 2, #red colour
cex = 4, #make it bigger!
lwd = 2) #make the cross lines thicker
plot(forward_summary$adjr2,
xlab = "# Predictors", #x-axis label
ylab = "AdjR2", #y-axis label
type = "l", #line plot
lwd = 2) #line thickness
adjr2_max = which.max(forward_summary$adjr2)
points(adjr2_max,
forward_summary$adjr2[adjr2_max],
pch = 4, #cross symbol used
col = 2, #red colour
cex = 4, #make it bigger!
lwd = 2) #make the cross lines thicker
View(HouseData)
names(HouseData)
lambdas = 10 ^ seq(-3, 3, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, c("bedrooms", "bathrooms", "sqft_living")]),
as.matrix(HouseData$price),
alpha = 0,
lambda = lambdas,
nfolds = 20
)
plot(cv_fit)
lambdas = 10 ^ seq(6, 13, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, c("bedrooms", "bathrooms", "sqft_living")]),
as.matrix(HouseData$price),
alpha = 0,
lambda = lambdas,
nfolds = 20
)
plot(cv_fit)
lambdas = 10 ^ seq(6, 13, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, !names(HouseData) %in% c("price")]),
as.matrix(HouseData$price),
alpha = 0,
lambda = lambdas,
nfolds = 20
)
plot(cv_fit)
lambdas = 10 ^ seq(6, 13, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, !names(HouseData) %in% c("price")]),
as.matrix(HouseData$price),
alpha = 1,
lambda = lambdas,
nfolds = 20
)
plot(cv_fit)
lambdas = 10 ^ seq(-13, 13, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, !names(HouseData) %in% c("price")]),
as.matrix(HouseData$price),
alpha = 1,
lambda = lambdas,
nfolds = 20
)
plot(cv_fit)
lambdas = 10 ^ seq(-13, 13, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, !names(HouseData) %in% c("price")]),
as.matrix(HouseData$price),
alpha = 1,
lambda = lambdas,
nfolds = 20,
thresh=1e-10
)
plot(cv_fit)
lambdas = 10 ^ seq(3, 13, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, !names(HouseData) %in% c("price")]),
as.matrix(HouseData$price),
alpha = 1,
lambda = lambdas,
nfolds = 20,
thresh=1e-10
)
plot(cv_fit)
lambdas = 10 ^ seq(3, 12, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, !names(HouseData) %in% c("price")]),
as.matrix(HouseData$price),
alpha = 1,
lambda = lambdas,
nfolds = 20,
thresh=1e-10
)
plot(cv_fit)
lambdas = 10 ^ seq(3, 5, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, !names(HouseData) %in% c("price")]),
as.matrix(HouseData$price),
alpha = 1,
lambda = lambdas,
nfolds = 20,
thresh=1e-10
)
plot(cv_fit)
lambdas = 10 ^ seq(3, 6, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, !names(HouseData) %in% c("price")]),
as.matrix(HouseData$price),
alpha = 1,
lambda = lambdas,
nfolds = 20,
thresh=1e-10
)
plot(cv_fit)
lambdas = 10 ^ seq(-3, 6, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, !names(HouseData) %in% c("price")]),
as.matrix(HouseData$price),
alpha = 1,
lambda = lambdas,
nfolds = 20,
thresh=1e-10
)
plot(cv_fit)
lambdas = 10 ^ seq(-3, 6, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, !names(HouseData) %in% c("price")]),
as.matrix(HouseData$price),
alpha = 1,
lambda = lambdas,
# nfolds = 20,
thresh=1e-10
)
plot(cv_fit)
lambdas = 10 ^ seq(-3, 5, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, !names(HouseData) %in% c("price")]),
as.matrix(HouseData$price),
alpha = 1,
lambda = lambdas,
# nfolds = 20,
thresh=1e-10
)
plot(cv_fit)
lambdas = 10 ^ seq(-1, 5, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, !names(HouseData) %in% c("price")]),
as.matrix(HouseData$price),
alpha = 1,
lambda = lambdas,
# nfolds = 20,
thresh=1e-10
)
plot(cv_fit)
lambdas = 10 ^ seq(0, 5, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, !names(HouseData) %in% c("price")]),
as.matrix(HouseData$price),
alpha = 1,
lambda = lambdas,
# nfolds = 20,
thresh=1e-10
)
plot(cv_fit)
lambdas = 10 ^ seq(1, 5, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, !names(HouseData) %in% c("price")]),
as.matrix(HouseData$price),
alpha = 1,
lambda = lambdas,
# nfolds = 20,
thresh=1e-10
)
plot(cv_fit)
lambdas = 10 ^ seq(2, 5, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, !names(HouseData) %in% c("price")]),
as.matrix(HouseData$price),
alpha = 1,
lambda = lambdas,
# nfolds = 20,
thresh=1e-10
)
plot(cv_fit)
lambdas = 10 ^ seq(-5, 5, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, !names(HouseData) %in% c("price")]),
as.matrix(HouseData$price),
alpha = 1,
lambda = lambdas,
# nfolds = 20,
thresh=1e-10
)
plot(cv_fit)
lambdas = 10 ^ seq(-5, 6, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, !names(HouseData) %in% c("price")]),
as.matrix(HouseData$price),
alpha = 1,
lambda = lambdas,
# nfolds = 20,
thresh=1e-10
)
plot(cv_fit)
lambdas = 10 ^ seq(-6, 6, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, !names(HouseData) %in% c("price")]),
as.matrix(HouseData$price),
alpha = 1,
lambda = lambdas,
# nfolds = 20,
thresh=1e-10
)
plot(cv_fit)
lambdas = 10 ^ seq(-6, 6, by = 0.1)
cv_fit = cv.glmnet(
as.matrix(HouseData[, !names(HouseData) %in% c("price")]),
as.matrix(HouseData$price),
alpha = 1,
lambda = lambdas,
# nfolds = 20,
thresh=1e-10
)
plot(cv_fit)
names(HouseData)
# Your data set
X <- c(1.890244, 3.194744, 2.645156, 1.773481, 2.872592, 2.247813, 2.624901, 2.813008, 2.296636, 2.183676)
# Create a scatter plot
plot(X, main = "Scatter Plot of X", xlab = "Index", ylab = "Value", pch = 19)
# Sample data
X <- c(1.890244, 3.194744, 2.645156, 1.773481, 2.872592, 2.247813, 2.624901, 2.813008, 2.296636, 2.183676)
# Function to calculate the sum of log p-values
log_likelihood_sum <- function(sample, mu, sigma) {
z_scores <- abs(sample - mu) / sigma
p_values <- 2 * pnorm(z_scores, lower.tail = FALSE)
log_p_values <- log(p_values)
sum(log_p_values)
}
# Estimate sample mean and standard deviation
mu_estimate <- mean(X)
sigma_estimate <- sd(X)
# Calculate the log likelihood sum for the estimated mu and sigma
log_likelihood <- log_likelihood_sum(X, mu_estimate, sigma_estimate)
print(paste("Estimated mu:", mu_estimate))
print(paste("Estimated sigma:", sigma_estimate))
print(paste("Log likelihood sum:", log_likelihood))
# Create a scatter plot
plot(
X,
main = "Scatter Plot of X",
xlab = "Index",
ylab = "Value",
pch = 19
)
x
X
mean(X)

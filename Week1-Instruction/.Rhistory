set.seed(111)
# lm method
print('------------------lm method------------------')
lm_model = lm(hp ~ mpg + drat + wt, data=mtcars)
coef(lm_model)
# glmnet
print('------------------GLMnet method------------------')
lambdas=10^seq(-3,3,by=0.1)
cv_fit = cv.glmnet(as.matrix(mtcars[,c("mpg","drat","wt")]),
as.matrix(mtcars$hp),
alpha=0,
lambda=lambdas,
nfolds=20)
cv_fit$lambda.min
coef(cv_fit, s="lambda.min")
predict_best_cv = predict(cv_fit, s="lambda.min",newx=as.matrix(mtcars[,c("mpg","drat","wt")]))
RMSE_best_cv = sqrt(mean((predict_best_cv-mtcars$hp)^2))
print(RMSE_best_cv)
set.seed(111)
# lm method
print('------------------lm method------------------')
lm_model = lm(hp ~ mpg + drat + wt, data=mtcars)
coef(lm_model)
# glmnet
print('------------------GLMnet method------------------')
lambdas=10^seq(-3,3,by=0.1)
cv_fit = cv.glmnet(as.matrix(mtcars[,c("mpg","drat","wt")]),
as.matrix(mtcars$hp),
alpha=0,
lambda=lambdas,
nfolds=20)
cv_fit$lambda.min
coef(cv_fit, s="lambda.min")
predict_best_cv = predict(cv_fit, s="lambda.min",newx=as.matrix(mtcars[,c("mpg","drat","wt")]))
RMSE_best_cv = sqrt(mean((predict_best_cv-mtcars$hp)^2))
print(RMSE_best_cv)
# CARet method
print('------------------CARet method------------------')
# lambdas=10^seq(-3,3,by=0.1)
ctrl_kfold = trainControl(method = "cv", number = 10)
ridge_kfold = train(hp ~ mpg+drat+wt,
data = mtcars,
method = "glmnet",
metric = "RMSE",
tuneGrid = expand.grid(alpha = 0,
lambda = lambdas),
trControl = ctrl_kfold,
thresh=1e-10)
print(ridge_kfold$finalModel$lambdaOpt)
coef(ridge_kfold$finalModel, ridge_kfold$bestTune$lambda)
prediction_ridge = predict(ridge_kfold,mtcars[,c("mpg","drat","wt")])
RMSE(prediction_ridge,mtcars$hp)
set.seed(111)
# lm method
print('------------------lm method------------------')
lm_model = lm(hp ~ mpg + drat + wt, data=mtcars)
coef(lm_model)
# glmnet
print('------------------GLMnet method------------------')
lambdas=10^seq(-3,3,by=0.1)
cv_fit = cv.glmnet(as.matrix(mtcars[,c("mpg","drat","wt")]),
as.matrix(mtcars$hp),
alpha=0,
lambda=lambdas,
nfolds=20)
cv_fit$lambda.min
coef(cv_fit, s="lambda.min")
predict_best_cv = predict(cv_fit, s="lambda.min",newx=as.matrix(mtcars[,c("mpg","drat","wt")]))
RMSE_best_cv = sqrt(mean((predict_best_cv-mtcars$hp)^2))
print(RMSE_best_cv)
# CARet method
print('------------------CARet method------------------')
# lambdas=10^seq(-3,3,by=0.1)
ctrl_kfold = trainControl(method = "cv", number = 10)
ridge_kfold = train(hp ~ mpg+drat+wt,
data = mtcars,
method = "glmnet",
metric = "RMSE",
tuneGrid = expand.grid(alpha = 0,
lambda = lambdas),
trControl = ctrl_kfold,
thresh=1e-10)
print(ridge_kfold$finalModel$lambdaOpt)
coef(ridge_kfold$finalModel, ridge_kfold$bestTune$lambda)
prediction_ridge = predict(ridge_kfold,mtcars[,c("mpg","drat","wt")])
RMSE(prediction_ridge,mtcars$hp)
set.seed(111)
# lm method
print('------------------lm method------------------')
lm_model = lm(hp ~ mpg + drat + wt, data=mtcars)
coef(lm_model)
# glmnet
set.seed(111)
print('------------------GLMnet method------------------')
lambdas=10^seq(-3,3,by=0.1)
cv_fit = cv.glmnet(as.matrix(mtcars[,c("mpg","drat","wt")]),
as.matrix(mtcars$hp),
alpha=0,
lambda=lambdas,
nfolds=20)
cv_fit$lambda.min
coef(cv_fit, s="lambda.min")
predict_best_cv = predict(cv_fit, s="lambda.min",newx=as.matrix(mtcars[,c("mpg","drat","wt")]))
RMSE_best_cv = sqrt(mean((predict_best_cv-mtcars$hp)^2))
print(RMSE_best_cv)
# CARet method
set.seed(111)
print('------------------CARet method------------------')
# lambdas=10^seq(-3,3,by=0.1)
ctrl_kfold = trainControl(method = "cv", number = 10)
ridge_kfold = train(hp ~ mpg+drat+wt,
data = mtcars,
method = "glmnet",
metric = "RMSE",
tuneGrid = expand.grid(alpha = 0,
lambda = lambdas),
trControl = ctrl_kfold,
thresh=1e-10)
print(ridge_kfold$finalModel$lambdaOpt)
coef(ridge_kfold$finalModel, ridge_kfold$bestTune$lambda)
prediction_ridge = predict(ridge_kfold,mtcars[,c("mpg","drat","wt")])
RMSE(prediction_ridge,mtcars$hp)
set.seed(111)
# lm method
print('------------------lm method------------------')
lm_model = lm(hp ~ mpg + drat + wt, data=mtcars)
coef(lm_model)
# glmnet
set.seed(111)
print('------------------GLMnet method------------------')
lambdas=10^seq(-3,3,by=0.1)
cv_fit = cv.glmnet(as.matrix(mtcars[,c("mpg","drat","wt")]),
as.matrix(mtcars$hp),
alpha=0,
lambda=lambdas,
nfolds=20,
thresh=1e-10)
cv_fit$lambda.min
coef(cv_fit, s="lambda.min")
predict_best_cv = predict(cv_fit, s="lambda.min",newx=as.matrix(mtcars[,c("mpg","drat","wt")]))
RMSE_best_cv = sqrt(mean((predict_best_cv-mtcars$hp)^2))
print(RMSE_best_cv)
# CARet method
set.seed(111)
print('------------------CARet method------------------')
# lambdas=10^seq(-3,3,by=0.1)
ctrl_kfold = trainControl(method = "cv", number = 10)
ridge_kfold = train(hp ~ mpg+drat+wt,
data = mtcars,
method = "glmnet",
metric = "RMSE",
tuneGrid = expand.grid(alpha = 0,
lambda = lambdas),
trControl = ctrl_kfold,
thresh=1e-10)
print(ridge_kfold$finalModel$lambdaOpt)
coef(ridge_kfold$finalModel, ridge_kfold$bestTune$lambda)
prediction_ridge = predict(ridge_kfold,mtcars[,c("mpg","drat","wt")])
RMSE(prediction_ridge,mtcars$hp)
set.seed(111)
# lm method
print('------------------lm method------------------')
lm_model = lm(hp ~ mpg + drat + wt, data=mtcars)
coef(lm_model)
# glmnet
set.seed(111)
print('------------------GLMnet method------------------')
lambdas=10^seq(-3,3,by=0.1)
cv_fit = cv.glmnet(as.matrix(mtcars[,c("mpg","drat","wt")]),
as.matrix(mtcars$hp),
alpha=0,
lambda=lambdas,
nfolds=10,
thresh=1e-10)
cv_fit$lambda.min
coef(cv_fit, s="lambda.min")
predict_best_cv = predict(cv_fit, s="lambda.min",newx=as.matrix(mtcars[,c("mpg","drat","wt")]))
RMSE_best_cv = sqrt(mean((predict_best_cv-mtcars$hp)^2))
print(RMSE_best_cv)
# CARet method
set.seed(111)
print('------------------CARet method------------------')
# lambdas=10^seq(-3,3,by=0.1)
ctrl_kfold = trainControl(method = "cv", number = 10)
ridge_kfold = train(hp ~ mpg+drat+wt,
data = mtcars,
method = "glmnet",
metric = "RMSE",
tuneGrid = expand.grid(alpha = 0,
lambda = lambdas),
trControl = ctrl_kfold,
nfolds=10,
thresh=1e-10)
print(ridge_kfold$finalModel$lambdaOpt)
coef(ridge_kfold$finalModel, ridge_kfold$bestTune$lambda)
prediction_ridge = predict(ridge_kfold,mtcars[,c("mpg","drat","wt")])
RMSE(prediction_ridge,mtcars$hp)
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
file = "auto-mpg.csv"
autompgCSV = read.csv(file,header=TRUE)
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
# current_directory
current_directory = getwd()
# read_csv
# joint file path
file_path = file.path(current_directory, "auto-mpg.csv")
autompgCSV = read.csv(file,header=TRUE)
file_path
read.csv(file,header=TRUE)
# read_csv
# joint file path
file_path = file.path(current_directory, "auto-mpg.csv")
autompgCSV = read.csv(file_path, header=TRUE)
head(autompgCSV)
View(autompgCSV)
sort(unique(autompgCSV$horsepower))
View(autompgCSV)
as.numeric(sort(unique(autompgCSV$horsepower)))
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
# current_directory
current_directory = getwd()
# read_csv
# joint file path
file_path = file.path(current_directory, "auto-mpg.csv")
autompgCSV = read.csv(file_path, header=TRUE)
head(autompgCSV)
sort(unique(autompgCSV$horsepower))
as.numeric(sort(unique(autompgCSV$horsepower)))
autompgCSV$horsepower = as.numeric(autompgCSV$horsepower)
head(autompgCSV)
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
# current_directory
current_directory = getwd()
# read_csv
# joint file path
file_path = file.path(current_directory, "auto-mpg.csv")
autompgCSV = read.csv(file_path, header=TRUE)
head(autompgCSV)
sort(unique(autompgCSV$horsepower))
as.numeric(sort(unique(autompgCSV$horsepower)))
autompgCSV$horsepower = as.numeric(autompgCSV$horsepower)
head(autompgCSV)
library(dplyr) # https://dplyr.tidyverse.org/index.html
autompg = autompgCSV %>% mutate(horsepower = as.numeric(horsepower))
head(autompg)
# Method 1 - using complete.cases and pipes
autompg %>% filter(!complete.cases(.))
#Method 2 - using complete.cases and base R
#autompg[!complete.cases(autompg),]
#Method 1 - using complete.cases and pipes
#autompg %>% filter(complete.cases(.))
#Method 2 - using complete.cases and base R
#autompg[complete.cases(autompg),]
#Method 3 - using na.omit
#na.omit(autompg)
#Method 4 - using na.omit and pipes
autompg %>% na.omit
autompgclean = autompg %>% na.omit %>% select(-name)
head(autompgclean)
library("ggplot2")
library("GGally")
ggpairs(autompgclean)+theme_bw()
# theme_bw is one of many themes you can modify from ggplot
# see https://ggplot2.tidyverse.org/reference/ggtheme.html for other themes
ols_model = lm(mpg ~ horsepower + displacement + weight + acceleration, data=autompgclean)
summary(ols_model)
library(caret)
library(glmnet)
lambdas=10^seq(-3,3,by=0.1)
ctrl_kfold = trainControl(method = "cv", number = 10)
ridge_kfold = train(mpg ~ horsepower + displacement + weight + acceleration,
data = autompgclean,
method = "glmnet",
metric = "RMSE",
tuneGrid = expand.grid(alpha = 0,
lambda = lambdas),
trControl = ctrl_kfold,
thresh=1e-10)
print(ridge_kfold$finalModel$lambdaOpt)
View(ridge_kfold)
coef(ridge_kfold$finalModel, ridge_kfold$bestTune$lambda)
head(ridge_kfold$results)
cv_ridgeplot = ggplot(ridge_kfold$results) + #choose the dataset that contains the variables we want to use
geom_point(aes(x = lambda, y = RMSE), color = "red") + #plot the RMSE for each lambda in red
scale_x_log10(labels = function(x) format(x, scientific = FALSE)) + #change the x-axis label to log scale and remove scientific notation
geom_errorbar(aes(x = lambda, ymin = RMSE - RMSESD, ymax = RMSE + RMSESD), width = 0.3, color = "lightgray") + #add errorbars using the CV standard deviation for the RMSE
theme_bw() #select the bw theme
cv_ridgeplot
Ridge_lambda = ridge_kfold$bestTune$lambda
RMSE_Ridge = ridge_kfold$results %>% filter(lambda == Ridge_lambda)
RMSE_Ridge
cv_ridgeplot +
geom_point(data=RMSE_Ridge, aes(x=lambda, y=RMSE), colour = "purple4", size = 3) +
geom_label(
data= RMSE_Ridge,
label="Winner",
aes(x=lambda, y=RMSE),
nudge_x = 0.25,
nudge_y = 0.25
)
lambdas=seq(0.01,2,by=0.01)
ctrl_kfold = trainControl(method = "cv", number = 10)
lasso_kfold = train(mpg ~ horsepower + displacement + weight + acceleration,
data = autompgclean,
method = "glmnet",
metric = "RMSE",
tuneGrid = expand.grid(alpha = 1,
lambda = lambdas),
trControl = ctrl_kfold,
thresh=1e-10)
print(lasso_kfold$finalModel$lambdaOpt)
coef(lasso_kfold$finalModel, lasso_kfold$bestTune$lambda)
cv_lassoplot = ggplot(lasso_kfold$results) + #choose the dataset that contains the variables we want to use
geom_point(aes(x = lambda, y = RMSE), color = "red") + #plot the RMSE for each lambda in red
#scale_x_log10(labels = function(x) format(x, scientific = FALSE)) + #change the x-axis label to log scale and remove scientific notation
geom_errorbar(aes(x = lambda, ymin = RMSE - RMSESD, ymax = RMSE + RMSESD), width = 0.3, color = "lightgray") + #add errorbars using the CV standard deviation for the RMSE
theme_bw() #select the bw theme
cv_lassoplot
lasso_lambda = lasso_kfold$bestTune$lambda
RMSE_lasso = lasso_kfold$results %>% filter(lambda == lasso_lambda)
RMSE_lasso
cv_lassoplot +
geom_point(data=RMSE_lasso, aes(x=lambda, y=RMSE), colour = "purple4", size = 3) +
geom_label(
data= RMSE_lasso,
label="Winner?",
aes(x=lambda, y=RMSE),
nudge_x = 0.1,
nudge_y = 0.1
)
?coef
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
NAME = "Zehao Qian"
COLLABORATORS = "Zehao Qian"
library(MASS)
data(Boston)
summary(lm(medv~rm+lstat+indus+ptratio, data=Boston))
summary(lm(medv~rm+lstat+indus+ptratio+nox, data=Boston))
summary(lm(medv~rm+lstat+indus+ptratio+zn, data=Boston))
m0=lm(medv~1, data=Boston)
summary(m0)
mfull=lm(medv~ . , data=Boston)
summary(mfull)
m_example=lm(medv ~ nox, data=Boston)
summary(m_example)
r2_example = summary(m_example)$r.squared
adjr2_example = summary(m_example)$adj.r.squared
print(paste0("R2 = ",r2_example))
print(paste0("Adj R2 = ",adjr2_example))
summary(lm(medv ~ ., data=Boston[,c(5,14)]))
r2_m1 = rep(NA,13)
for (var in 1:13){
m_temp = lm(medv~ . , data = Boston[,c(var,14)])
r2_m1[var]=summary(m_temp)$r.squared
}
print(r2_m1)
print(paste0("The maximum calcuated R2 is: ", max(r2_m1)))
print(paste0("The index of the corresponding model is: ",which.max(r2_m1)))
print(paste0("The relevant predictor is: ", names(Boston)[var]))
all_pairs = combn(13,2)
View(all_pairs)
summary(lm(medv ~ ., data = Boston[,c(all_pairs[,1],14)]))
r2_m2 = rep(NA,ncol(all_pairs))
for (i in 1:ncol(all_pairs)) {
m_temp2 = summary(lm(medv ~ ., data = Boston[,c(all_pairs[,i],14)]))
r2_m2[i]=m_temp2$r.squared
}
# r2_m2_max = which.max(r2_m2)
# r2_m2_max
# print(r2_m2)
print(paste0("The maximum calcuated R2 is: ", max(r2_m2)))
print(paste0("The index of the corresponding model is: ",which.max(r2_m2)))
library(leaps)
best_models = regsubsets(medv ~ ., data=Boston)
summary(best_models)
res_summary = summary(best_models)
names(res_summary)
which.max(res_summary$adjr2)
print(coef(best_models,8))
summary(lm(reformulate(paste(names(coef(best_models,8))[-1],collapse="+"),'medv'),data=Boston))
library(leaps)
best_forward = bss_fit = regsubsets(medv ~ ., data=Boston, method="forward")
summary(best_forward)
forward_summary = summary(best_forward)
forward_adjr2=which.max(forward_summary$adjr2)
print(coef(best_forward,forward_adjr2))
summary(lm(reformulate(paste(names(coef(best_forward,forward_adjr2))[-1],collapse="+"),'medv'),data=Boston))
# ---------------------------------------------------------------------
# Best Subset Selection methods
library(leaps)
best_subset_selection = regsubsets(mpg ~ ., data=autompgclean)
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
# current_directory
current_directory = getwd()
# read_csv
# joint file path
file_path = file.path(current_directory, "auto-mpg.csv")
autompgCSV = read.csv(file_path, header=TRUE)
head(autompgCSV)
sort(unique(autompgCSV$horsepower))
as.numeric(sort(unique(autompgCSV$horsepower)))
autompgCSV$horsepower = as.numeric(autompgCSV$horsepower)
head(autompgCSV)
library(dplyr) # https://dplyr.tidyverse.org/index.html
autompg = autompgCSV %>% mutate(horsepower = as.numeric(horsepower))
head(autompg)
# Method 1 - using complete.cases and pipes
autompg %>% filter(!complete.cases(.))
# Method 2 - using complete.cases and base R
# autompg[!complete.cases(autompg),]
#Method 1 - using complete.cases and pipes
#autompg %>% filter(complete.cases(.))
#Method 2 - using complete.cases and base R
#autompg[complete.cases(autompg),]
#Method 3 - using na.omit
#na.omit(autompg)
#Method 4 - using na.omit and pipes
autompg %>% na.omit
autompgclean = autompg %>% na.omit %>% select(-name)
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
# current_directory
current_directory = getwd()
# read_csv
# joint file path
file_path = file.path(current_directory, "auto-mpg.csv")
autompgCSV = read.csv(file_path, header=TRUE)
head(autompgCSV)
sort(unique(autompgCSV$horsepower))
as.numeric(sort(unique(autompgCSV$horsepower)))
autompgCSV$horsepower = as.numeric(autompgCSV$horsepower)
head(autompgCSV)
library(dplyr) # https://dplyr.tidyverse.org/index.html
autompg = autompgCSV %>% mutate(horsepower = as.numeric(horsepower))
head(autompg)
# Method 1 - using complete.cases and pipes
autompg %>% filter(!complete.cases(.))
# Method 2 - using complete.cases and base R
# autompg[!complete.cases(autompg),]
#Method 1 - using complete.cases and pipes
#autompg %>% filter(complete.cases(.))
#Method 2 - using complete.cases and base R
#autompg[complete.cases(autompg),]
#Method 3 - using na.omit
#na.omit(autompg)
#Method 4 - using na.omit and pipes
autompg %>% na.omit
autompgclean = autompg %>% na.omit %>% select(-name)
# ---------------------------------------------------------------------
# Best Subset Selection methods
library(leaps)
best_subset_selection = regsubsets(mpg ~ ., data=autompgclean)
# ---------------------------------------------------------------------
# Best Subset Selection methods
library(leaps)
best_subset_selection = regsubsets(mpg ~ ., data=autompgclean)

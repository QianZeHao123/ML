Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\rightarrow$Run All).

Make sure you fill in any place that says `YOUR CODE HERE` or "YOUR ANSWER HERE", as well as your name and collaborators below:

```{r}
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
```

```{r}
NAME = "Zehao Qian"
COLLABORATORS = "Zehao Qian"
```

------------------------------------------------------------------------

## Machine Learning - Workshop 2

### Preliminaries

**Aim**: In this workshop we will explore methods for model selection and the gradient descent algorithm.

**Preparation**: Take a look at the contents for weeks 7 and 8 in Introduction to Statistics. We will build on some of the concepts you have already seen such as variable selection and model validation.

### Best Subset Selection

#### **Introduction**

In Workshop 1, we used the *Boston* dataset to fit a linear model using **lm**. We had *medv* as our response variable, and the following variables as our predictors: *rm*, *lstat*, *indus*, and *ptratio*.

```{r}
library(MASS)
data(Boston)
summary(lm(medv~rm+lstat+indus+ptratio, data=Boston))
```

```         
Call:
lm(formula = medv ~ rm + lstat + indus + ptratio, data = Boston)

Residuals:
     Min       1Q   Median       3Q      Max 
-14.5602  -3.1379  -0.7984   1.7783  29.5739 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 18.614970   3.926680   4.741 2.78e-06 ***
rm           4.515179   0.426286  10.592  < 2e-16 ***
lstat       -0.575711   0.047885 -12.023  < 2e-16 ***
indus        0.007567   0.043594   0.174    0.862    
ptratio     -0.935122   0.120464  -7.763 4.71e-14 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 5.234 on 501 degrees of freedom
Multiple R-squared:  0.6786,    Adjusted R-squared:  0.6761 
F-statistic: 264.5 on 4 and 501 DF,  p-value: < 2.2e-16
```

We can see that the $R^2$ for this model is 0.6786. This means that 67.86% of the variation in the quantitative measure of property median value (medv) can be explained by its linear regression on the 4 chosen predictor variables. We can also see that the adjusted $R^2$ is 0.6761.

Say we now include the *nox* predictor in our model:

```{r}
summary(lm(medv~rm+lstat+indus+ptratio+nox, data=Boston))
```

The $R^2$ has moved from 67.61% to 67.99%. So, if we assume that a model with a higher $R^2$ is better, we would choose this model over the previous one. In this case, the adjusted $R^2$ is also higher at 0.6767.

**Question:** Which measure should we use to compare the first and the second model? $R^2$ or the adjusted $R^2$? Discuss this with a colleague or a tutor before proceeding.

SImilarly, if we add the variable *zn* to the first model, we will see a decrease in the adjusted $R^2$ in comparison to the first model. We can also see a decrease in the $R^2$ in comparison to the second model. Therefore, we would (possibly) conclude that we are better off with the second model.

```{r}
summary(lm(medv~rm+lstat+indus+ptratio+zn, data=Boston))
```

If we are to consider all possible linear combinations of variables, we would need to consider the model with no predictors (only an intercept) and the model with all predictors available. Let's see what the calculated $R^2$ and adjusted $R^2$ is for these models:

```{r}
m0=lm(medv~1, data=Boston)
summary(m0)
```

```{r}
mfull=lm(medv~ . , data=Boston)
summary(mfull)
```

**Question**: What happened to the $R^2$ and adjusted $R^2$ for the model $M_0$ above? Again discuss this with your colleagues or a tutor.

#### **Best Subset Selection procedure**

To perform best subset selection, we fit a separate regression model for each possible combination of the $p$ predictors. This is often broken up into stages, as follows:

1.  Let $M_0$ denote the model which contains no predictors.

2.  For $k=1,2,\ldots,p$:

-   Fit all $p \choose k$ models that contain exactly $k$ predictors.
-   Pick the best among these models and call it $M_k$. Here, best is defined as having the smallest RSS or largest $R^2$.

3.  Select a single best model from $M_0,M_1,\ldots,M_p$ using a suitable measure such as the cross-validated prediction error $C_p$, BIC, adjusted $R^2$, etc.

It is important to note that use of RSS or $R^2$ in step 2 of the above algorithm is acceptable because the models all have an equal number of predictors. We can't use RSS in step 3 because RSS decreases monotonically as the number of predictors included in the model increases.

**Exercise**: Let's write a loop to try to find the best subset model for *medv*.

*Note*: THere are many different ways to complete this exercise using libraries, data subsetting/manipulation, etc. The way we are going to attempt this now is not optimal but it is simple and easy to implement.

**Step 1:** Say we want to fit the model *medv \~ nox*, we write:

```{r}
m_example=lm(medv ~ nox, data=Boston)
summary(m_example)
```

**Step 2:** And we can get the $R^2$ and adjusted $R^2$ for this model by calling:

```{r}
r2_example = summary(m_example)$r.squared
adjr2_example = summary(m_example)$adj.r.squared
print(paste0("R2 = ",r2_example))
print(paste0("Adj R2 = ",adjr2_example))
```

**Step 3:** When we called the model with all variables in the previous section, we used *lm(medv \~ ., data=Boston)*. The dot indicates we want to use all variables in the dataset Boston.

Instead of writing *lm(medv \~ nox, data=Boston)* in the previous model, we could write *lm(medv \~ ., data=Boston[,c(5,14)])*.

The 5th column in Boston corresponds to the variable *nox* and the last column (14th) corresponds to *medv*. So we have subsetted the dataset keeping only the columns of interest for this model.

```{r}
summary(lm(medv ~ ., data=Boston[,c(5,14)]))
```

**Step 4:** Putting everything together, we can write a simple loop to compare all models containing one variable, and saving their $R^2$ to a vector.

```{r}
r2_m1 = rep(NA,13)
for (var in 1:13){
  m_temp = lm(medv~ . , data = Boston[,c(var,14)])
  r2_m1[var]=summary(m_temp)$r.squared
}
print(r2_m1)
print(paste0("The maximum calcuated R2 is: ", max(r2_m1)))
print(paste0("The index of the corresponding model is: ",which.max(r2_m1)))
print(paste0("The relevant predictor is: ", names(Boston)[var]))
```

**Step 5:** Say we now want to look at all models with 2 variables, we would have to find all pairwise combinations of predictors. To do this, we can use the function *combn*:

```{r}
all_pairs = combn(13,2)
View(all_pairs)
```

**Step 6:** And we can use the same principle to subset the Boston dataset as before to generate the model using the variables in column 1 of *all_pairs*:

```{r}
summary(lm(medv ~ ., data = Boston[,c(all_pairs[,1],14)]))
```

**Step 7:** Now write a loop similar to the one in **Step 4** and find the model with the highest $R^2$ with two predictors.

```{r}
r2_m2 = rep(NA,ncol(all_pairs))
for (i in 1:ncol(all_pairs)) {
  m_temp2 = summary(lm(medv ~ ., data = Boston[,c(all_pairs[,i],14)]))
  r2_m2[i]=m_temp2$r.squared
}
# r2_m2_max = which.max(r2_m2)
# r2_m2_max
# print(r2_m2)
print(paste0("The maximum calcuated R2 is: ", max(r2_m2)))
print(paste0("The index of the corresponding model is: ",which.max(r2_m2)))
```

**Step 8:** Repeat steps 5 to 7 and create a loop that returns the model with the highest value of $R^2$ for $1, 2, \ldots, 13$ variables. Find a strategy to save the index for the relevant variables in each model, the corresponding $R^2$, and the adjusted $R^2$ for each.

```{r}

```

**Step 9:** Compare the adjusted $R^2$ for the models in **Step 8** and identify the best subset model.

```{r}
```

### **Painless best subset selection**

Now that you have seen one way to implement subset selection (other solutions exist!), you should be ready to use functions that do this job for you without the need to implement your own loops.

We use the library **leaps** to return the best subset model.

```{r}
library(leaps)
best_models = regsubsets(medv ~ ., data=Boston)
summary(best_models)
```

The *regsubsets* function returns 8 possible "best subset" models. To check which one is which, you can use the *names* function:

```{r}
res_summary = summary(best_models)
names(res_summary)
```

And to identify the model with the highest adjusted $R^2$, we use the function **which.max**:

```{r}
which.max(res_summary$adjr2)
```

And the coefficients for model 8 are:

```{r}
print(coef(best_models,8))
```

```{r}
summary(lm(reformulate(paste(names(coef(best_models,8))[-1],collapse="+"),'medv'),data=Boston))
```

### Forwards and Backwards Subset Selection

Forwards and backwards subset selection are similar techniques which do not make use of every possible combination of variables.

**Forward selection**

In the case of forwards subset selection, we start with the null model, and determine which of the $p$ single-predictor models has the highest $R^2$ value. We designate this model $M_1$.

Next, rather than fitting each of the $p \choose 2$ two-predictor models, we add each of the remaining $p-1$ predictor variables to $M_1$ and determine which of these models has the highest $R^2$ value. We then designate this model $M_2$.

We continue in this fashion until we have our set of $p$ models $M_0,M_1,\ldots,M_p$, from which we choose the "best" model using $C_p$, BIC, adjusted $R^2$, etc.

**Backward selection**

In the case of backwards subset selection, we start from the *full* model $M_p$ and *remove* a single variable at a time.

The model from this set of $p$ models which has the highest $R^2$ value is designated $M_{p-1}$. We continue in this fashion until we reach the null model, $M_0$. We again choose the best model from $M_0,M_1,\ldots,M_p$ based on $C_p$, BIC, adjusted $R^2$, etc.

#### Implementation using Leaps

We can use the **leaps** package and *regsubsets* to conduct forward and backward selection, we just need to specify the method. To perform forward selection and select the best model using the adjust $R^2$, we include our method choice in the function call as follows.

```{r}
library(leaps)
best_forward = bss_fit = regsubsets(medv ~ ., data=Boston, method="forward")
summary(best_forward)
```

```{r}
forward_summary = summary(best_forward)
forward_adjr2=which.max(forward_summary$adjr2)
print(coef(best_forward,forward_adjr2))
```

```{r}
summary(lm(reformulate(paste(names(coef(best_forward,forward_adjr2))[-1],collapse="+"),'medv'),data=Boston))
```

In this case, the forward selection process leads to the same model as the one obtained using best subset selection.

**Exercise:** Repeat the process above using *backward* as the method option. How different is this model from the best subset and forward models?

```{r}
```

### Gradient Descent Algorithms

The model selection methods you have seen today are a form of optimization. We wanted to find potentially optimal sets of variables that maximixed the $R^2$ and the adjusted $R^2$. We could have chosen to minimize the RSS, or use a different selection criteria. The key is that we were trying to optimize with respect to a specific cost function (e.g. $R^2$, RSS, BIC).

The best subset selection method is effective but computationally intensive; it becomes unsustainable as the number of features grow and the number of combinations grow exponentially. We then start to use other methods such as stepwise forward and backward regression. But there is a point when those also become unreliable either due to model complexity or data volume. At that point, other algorithms that are more commonly used for optimization of continuous functions may be used.

Here we will briefly introduce the gradient descent algorithm. It is widely used to support the fit of neural networks and other machine learning models. Here we will implement a simple version of a gradient descent.

Before you move to the implementation below, take a look at a brief explanation of gradient descent at the [Khan academy](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/optimizing-multivariable-functions/a/what-is-gradient-descent).

**Example**

For our implementation, we will assume we want to find a local minimum for

$$f(x_1,x_2,x_3)=\cos(5x_1)+\sin(3x_2)+(x_3-4)(x_3+3).$$

The gradient descent algorithm consists of taking steps in the opposite direction of the gradient of the function at a point. Instead if we wanted to find the maximum, we would move on the direction of the gradient and that is called a gradient ascent.

The function above isn't one we can easily visualize like the ones in the Khan academy post, but we can take a look at its contours for different values of $x_3$. In the plot below, we have $x_3=1$. Change the value of $x_3$ and check what happens to the heatmap.

```{r}
my_function = function(x1,x2,x3){
  return(cos(5*x1) + sin(3*x2) + (x3-4)*(x3+3))
}
```

```{r}
a = expand.grid(seq(-2,2,length.out=20),seq(-2,2,length.out=20))
x_3 = 1
filled.contour(x = seq(-2,2,length.out=20),
               y = seq(-2,2,length.out=20),
               z = matrix(my_function(a[,1],a[,2],x_3), 20),
               plot.title = title(main = paste0("x_3 = ",x_3)),
               plot.axes = { axis(1); axis(2); points(10, 10) })
```

We could calculate the partial derivatives for $f$ with respect to each variable. However, this isn't feasible for all functions even if continuous nor it is always efficient to compute an analytic version of the derivative for every function being explored. .

Remember that we can write a derivative as a limit

$$
\frac{\delta f(x)}{\delta x}=\lim_{h\rightarrow 0}\frac{f(x+h)-f(x)}{h}
$$

and for a given $h$ (very small) we can numerically approximate the partial derivatives for our function $f$ on a given value $x$ using the function below.

**Exercise:** Annotate the function *partial* below explaining what each part of the function is doing and what you expect it to return.

```{r}
partial <- function(x,h){
  x1 <- x[1]
  x2 <- x[2]
  x3 <- x[3]
  value <- my_function(x1,x2,x3)
  partial1 <- (my_function(x1+h,x2,x3) - value)/h
  partial2 <- (my_function(x1,x2+h,x3) - value)/h
  partial3 <- (my_function(x1,x2,x3+h) - value)/h
  partialderiv <- c(partial1,partial2,partial3)
  return(partialderiv)
}
```

**Exercise:** The gradient descent algorithm moves in the opposite direction of the function's gradient at a given value. The function *calc_xnew* below takes the current position $x$, the computed values for the partial derivatives at $x$ and the multiplier $eps$ that indicates how far (and fast) the algorithm should move from $x$.

```{r}
calc_xnew <- function(x,partialderiv,eps){
  xnew <- x - eps*partialderiv
  return(xnew)
}
```

We can now put everything together and set relevant stopping criteria such as the maximum number of iterations and the distance between points.

**Exercise:** Comment the function below explaining each step and reflecting on the choices of stopping criteria below. Try changing the values for *max_iter* and *e_diff*, as well as the starting values for the function **find_argmin**.

```{r}
find_argmin = function(x0,h,eps){
  max_iter = 100000
  e_diff=1.e-8
  iterations = 0
  converged = FALSE
  xnew = x0
  while(converged == FALSE){
    partialderiv = partial(xnew,h)
    xold=xnew
    xnew = calc_xnew(xnew,partialderiv,eps)
    #stopping criteria based on distance between points
    if(abs(sum((xnew-xold)**2)) < e_diff){
      converged = TRUE
      return(paste("ArgMin of non-linear function: ",xnew[1],xnew[2],xnew[3]))
    }

    iterations = iterations + 1
    #stopping criteria based on number of iterations
    if(iterations > max_iter){
      return(paste(abs(sum((xnew-xold)**2)),"Too many iterations!"))
      converged = TRUE
    }
  }
}

find_argmin(c(10,2,5),0.001,0.2)
```

```{r}
```

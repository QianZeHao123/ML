#### MATH42815 - Machine Learning - 2023/24

## Machine Learning - Workshop 5

**Aim**: In this workshop we will explore another regularization technique, the LASSO: Least Absolute Shrinkage and Selection Operator. Yes, statisticians and data scientists do like to come up with terrible acronyms, you should start crafting your own.

### **L1 and L2 Regularization**

In the last workshop, we introduced the Ridge regression as a method of estimating the coefficients of multiple-regression models in scenarios where the independent variables are highly correlated. It uses what we call $L2$ regularization as the penalty term is proportional to the sum of the squares of the model coefficients. In short, it estimates the significance of the features in a model and penalizes "insignificant" ones. Larger coefficients are penalized more than small ones leading to smaller coefficient values and potentially lower variance overall.

The LASSO (Least Absolute Shrinkage and Selection Operator) uses a $L1$ penalty for fitting and penalizing the coefficients. The $L1$ penalty is proportional to the sum of the absolute values of the model coefficients.

In L1 regularization, the coefficients of the less important features in the model are often shrunk to zero reducing the dimensionality of the model. L1 regression can therefore be used for model selection.

Later on we will use L1 and L2 regularizations together to calibrate neural networks.

In the last workshop, we compared the matrix form estimation for least squares and ridge coefficients. In this workshop, we will focus on the optimization problem that each method addresses.

We start with our linear model

$$y=\beta X + \epsilon$$

where $y$ is the response vector, $X$ is the design matrix, $\beta$ is the vector of coefficients we want to estimate, and $\epsilon$ is the error vector.

For ordinary least squares, we want to compute

$$\min_{\beta} ||\epsilon||^2 = \min_{\beta} ||y-X\beta||^2.$$

That is, we want to find $\beta$ such that the the sum of the errors squared is minimum. This leads to the closed form

$$\hat{\beta}_{LOS} = (X^TX)^{-1}X^Ty.$$

For the ridge regression, we penalize our objective function and write our problem as:

$$\min_{\beta} ||y-X\beta||^2 + \lambda_1||\beta||^2.$$

Similarly, for the the LASSO, we write:

$$\min_{\beta} ||y-X\beta||^2 + \lambda_2||\beta||_1.$$

If we combine them all, we get what is called an elastic net and we write:

$$\min_{\beta} ||y-X\beta||^2 + \lambda_1||\beta||^2 + \lambda_2||\beta||_1.$$

When we used the **glmnet** to fit our models, we had two parameters to set, *alpha* and *lambda*. The *alpha* comes from a rewriting of the objective function above. We set

$$\alpha = \frac{\lambda_2}{\lambda_1+\lambda_2}$$

and

$$\min_{\beta} ||y-X\beta||^2 + (1-\alpha)||\beta||^2 + \alpha||\beta||_1.$$

So when $\alpha=0$, we have the Ridge regression and when $\alpha = 1$, we have the LASSO.

### **LASSO with GLMnet**

For this workshop, we will use the *auto-mpg* dataset from the [UC Irvine Machine Learning repository](https://archive.ics.uci.edu/dataset/9/auto+mpg).

The *auto-mpg* dataset is similar to the *mtcars* dataset we used in the previous workshop but with far more observations. It contains information on fuel comsumption 8 other related variables for 398 automobiles.

Let's load the data and create a pairs plot of the data:

```{r}
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
# current_directory
current_directory = getwd()
```

```{R}
# read_csv
# joint file path
file_path = file.path(current_directory, "auto-mpg.csv")
autompgCSV = read.csv(file_path, header=TRUE)
```

```{R}
head(autompgCSV)
```

Let's clean this dataset a bit. Note that *horsepower* seems to be a *character* variable in the dataframe. That can't be right, let's see what is going on:

```{R}
sort(unique(autompgCSV$horsepower))
```

It looks like a question mark has been added to represent missing values which is causing all values of *horsepower* to be seen as strings instead of numbers. In this case, we can fix this problem by using the **as.numeric** function and then dealing with the missing values. When you apply the **as.numeric** function to a string vector, it'll try to convert all strings to numbers and, when a number is not returned, it will intrduce **NA**s by coercion. **NA** means not available.

```{R}
as.numeric(sort(unique(autompgCSV$horsepower)))
```

```{R}
autompgCSV$horsepower = as.numeric(autompgCSV$horsepower)
head(autompgCSV)
```

#### **Pipes in R**

You can also use pipes to perform the same action. Pipes are used to chain multiple operations together in a simple and concise way. Originally pipes were part of the **tidy** universe but now it is also available in more recent versions of base R. Here in this datacamp article ([link here](https://www.datacamp.com/tutorial/pipe-r-tutorial)) you can read a bit about the history of pipes in R.

Packages such as **magrittr**, **dplyr**, **tidyr**, and **purrr** will open the world of pipes in R to you. Pipes in R are normally written using %\>% but newer versions of R also take the F# pipes \|\> .

**Note:** you don't have to use pipes if you don't like them but you will often find them in data science tutorials, libraries, etc. so it is good to be aware of how they work.

```{R}
library(dplyr) # https://dplyr.tidyverse.org/index.html
autompg = autompgCSV %>% mutate(horsepower = as.numeric(horsepower))
head(autompg)
```

We also want to remove the rows with incomplete data from this dataset, let's see how we can identify them. Uncomment the examples in each method to see that the output for each method is the same:

```{R}
# Method 1 - using complete.cases and pipes
autompg %>% filter(!complete.cases(.))

# Method 2 - using complete.cases and base R
# autompg[!complete.cases(autompg),]
```

There are a number of ways of removing the rows with NAs, here are a few:

```{R}
#Method 1 - using complete.cases and pipes
#autompg %>% filter(complete.cases(.))

#Method 2 - using complete.cases and base R
#autompg[complete.cases(autompg),]

#Method 3 - using na.omit
#na.omit(autompg)

#Method 4 - using na.omit and pipes
autompg %>% na.omit
```

Let's save one of these to the dataframe **autompgclean**. We will also remove the **name** column from our dataset.

```{R}
autompgclean = autompg %>% na.omit %>% select(-name)
head(autompgclean)
```

```{R}
library("ggplot2")
library("GGally")
ggpairs(autompgclean)+theme_bw()

# theme_bw is one of many themes you can modify from ggplot
# see https://ggplot2.tidyverse.org/reference/ggtheme.html for other themes
```

Say we want to model *mpg* (miles per gallon) using the predictors *horsepower*, *displacement*, *weight* and *acceleration*. If we are using least squares, we use the **lm** function:

```{R}
ols_model = lm(mpg ~ horsepower + displacement + weight + acceleration, data=autompgclean)
summary(ols_model)
```

To fit a Ridge regression using **caret** and **glmnet** and try to find the best value of *lambda*, we use the process from Workshop 4:

```{R}
library(caret)
library(glmnet)
```

```{R}
lambdas=10^seq(-3,3,by=0.1)
ctrl_kfold = trainControl(method = "cv", number = 10)
ridge_kfold = train(mpg ~ horsepower + displacement + weight + acceleration,
              data = autompgclean,
              method = "glmnet",
              metric = "RMSE",
              tuneGrid = expand.grid(alpha = 0,
                                     lambda = lambdas),
              trControl = ctrl_kfold,
              thresh=1e-10)
print(ridge_kfold$finalModel$lambdaOpt)
```

```{R}
coef(ridge_kfold$finalModel, ridge_kfold$bestTune$lambda)
```

```{R}
head(ridge_kfold$results)
```

Let's use **ggplot** to recreate the plot we generated in the previous workshop when using **cv.glmnet**. **GGplot** uses an additive language to build plots. You can save them to variables in your workspace and keep modifying them until you land on a plot that shows what you need:

```{R}
cv_ridgeplot = ggplot(ridge_kfold$results) + #choose the dataset that contains the variables we want to use
  geom_point(aes(x = lambda, y = RMSE), color = "red") + #plot the RMSE for each lambda in red
  scale_x_log10(labels = function(x) format(x, scientific = FALSE)) + #change the x-axis label to log scale and remove scientific notation
  geom_errorbar(aes(x = lambda, ymin = RMSE - RMSESD, ymax = RMSE + RMSESD), width = 0.3, color = "lightgray") + #add errorbars using the CV standard deviation for the RMSE
  theme_bw() #select the bw theme

cv_ridgeplot
```

R has a large number of colour names built-in and you can always create your own by specifying their RGB, HSV, or HEX code. Here is a [list of all colours](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) available without a library.

Now let's add the "best" value of lambda to the plot:

```{R}
Ridge_lambda = ridge_kfold$bestTune$lambda
RMSE_Ridge = ridge_kfold$results %>% filter(lambda == Ridge_lambda)
RMSE_Ridge
cv_ridgeplot +
  geom_point(data=RMSE_Ridge, aes(x=lambda, y=RMSE), colour = "purple4", size = 3) +
  geom_label(
    data= RMSE_Ridge,
    label="Winner",
    aes(x=lambda, y=RMSE),
    nudge_x = 0.25,
    nudge_y = 0.25
  )
```

Now let's set *alpha* to 1 and see what the LASSO regression results look like:

```{R}
lambdas=seq(0.01,2,by=0.01)
ctrl_kfold = trainControl(method = "cv", number = 10)
lasso_kfold = train(mpg ~ horsepower + displacement + weight + acceleration,
              data = autompgclean,
              method = "glmnet",
              metric = "RMSE",
              tuneGrid = expand.grid(alpha = 1,
                                     lambda = lambdas),
              trControl = ctrl_kfold,
              thresh=1e-10)
print(lasso_kfold$finalModel$lambdaOpt)
```

```{R}
coef(lasso_kfold$finalModel, lasso_kfold$bestTune$lambda)
```

**Question:** What do the coefficients above tell you?

<!--# The acceleration has no doantion. -->

```{R}
cv_lassoplot = ggplot(lasso_kfold$results) + #choose the dataset that contains the variables we want to use
  geom_point(aes(x = lambda, y = RMSE), color = "red") + #plot the RMSE for each lambda in red
  #scale_x_log10(labels = function(x) format(x, scientific = FALSE)) + #change the x-axis label to log scale and remove scientific notation
  geom_errorbar(aes(x = lambda, ymin = RMSE - RMSESD, ymax = RMSE + RMSESD), width = 0.3, color = "lightgray") + #add errorbars using the CV standard deviation for the RMSE
  theme_bw() #select the bw theme

cv_lassoplot
```

```{R}
lasso_lambda = lasso_kfold$bestTune$lambda
RMSE_lasso = lasso_kfold$results %>% filter(lambda == lasso_lambda)
RMSE_lasso
cv_lassoplot +
  geom_point(data=RMSE_lasso, aes(x=lambda, y=RMSE), colour = "purple4", size = 3) +
  geom_label(
    data= RMSE_lasso,
    label="Winner?",
    aes(x=lambda, y=RMSE),
    nudge_x = 0.1,
    nudge_y = 0.1
  )
```

The LASSO model selected by **caret** with the lowest RMSE only has 3 variables with non-zero coefficients indicating that the variable *acceleration* was excluded from the final model. This agrees with the idea that $L1$ regularization can be used for model selection.

**Exercise:** Apply best subset selection, forward selection, and backward selection to the linear model introduced at the beginning. Which features were selected using each method?

```{R}
# ---------------------------------------------------------------------
# Best Subset Selection methods
library(leaps)
best_subset_selection = regsubsets(mpg ~ ., data=autompgclean)
summary(best_subset_selection)
```

**Exercise:** The parameter *alpha* takes values in $[0,1]$. We have explored the cases where *alpha* is 0 (ridge) and 1 (lasso). Build a workflow to explore both *alpha* and *lambda* at the same time and find a suitable elastic net to model *mpg*.

Can you think of a good way to present and explain your results?

*Note:* Choose an appropriately coarse grid of parameters to test your code before deploying a finer grid using *expand.grid*.

```{R}

```

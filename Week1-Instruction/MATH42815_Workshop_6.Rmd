#### MATH42815 - Machine Learning - 2023/24

## Machine Learning - Workshop 6

**Aim**: The aim of this workshop is to go through the basics of piecewise regression, polynomial regression and splines.

### **Piecewise Regression**

Up to now we have worked with models that fit a single line or hyperplane through a set of features.

Piecewise regression is a type of regression analysis that splits the data into different segments and fits a separate function to each segment. This can help capture changes or discontinuities in the relationship between the response variables and features.

For example, you can use piecewise regression to model inflation before and after changes in government, life expectancy before and after a pandemic, etc.

To perform piecewise regression, you need to specify the number and location of the breakpoints, which are the values of the response variable where the segments connect. You can also use an interaction term with a dummy variable to indicate which segment the data belongs to.

However, before diving straight into a piecewise regression model, it might be useful to explore whether another feature might be driving the changes you see in your data. For example, if you were trying to model life expectancy worldwide, you would likely take gender into account as, in average, women tend to outlive men by around 5 years. So modelling those subpopulations as separate groups with gender as a factor in your regression would be reasonable.

Let's move back to cars and load the auto-mpg dataset we have seen in workshops 4 and 5.

```{r}
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
# current_directory
current_directory = getwd()
# read_csv
# joint file path
file_path = file.path(current_directory, "auto-mpg.csv")
autompg = read.csv(file_path, header=TRUE)
```

```{r}
library(dplyr)
autompgclean = autompg %>% mutate(horsepower = as.numeric(horsepower)) %>% na.omit %>% select(-name)
```

```{r}
library("ggplot2")
library("GGally")
ggpairs(autompgclean)+theme_bw()
```

Say we are interested in modelling *mpg* as a function of *horsepower*. You can add a regression line to a **ggplot** by using **stat_smooth** as below.

```{r}
ggplot(autompgclean, aes(x=horsepower, y=mpg))+
  geom_point()+
  stat_smooth(method=lm,formula = y ~ x)+
  theme_bw()
```

We can see that there is a non-linear trend in the data which we will explore later on. For now, let's color the points using the variable *cylinders*:

```{r}
ggplot(autompgclean, aes(x=horsepower, y=mpg))+
  geom_point(aes(colour=factor(cylinders)))+
  stat_smooth(method=lm,formula = y ~ x)+
  theme_bw()
```

We can see that the vehicles with a higher number of cylinders produce more power but also have a lower mileage per gallon. You could treat cylinders as a numeric variable or as a factor; it is worth noting that while cylinders take values 3 to 8 in this dataset, the odd-valued cases are rare.

Let's try fitting a linear model with *cylinders* as an interaction factor:

```{r}
summary(lm(mpg ~ horsepower*factor(cylinders), data = autompgclean))
```

We have fitted a model that can be written roughly as:

$$mpg = 1.5161 + 0.1918 hp + \cases{
  0, & if cylinders = 3\\
  45.8468-0.4227 hp,& if cylinders = 4\\
58.5217 - 0.5886 hp, & if cylinders = 5\\
18.1007 - 0.1883 hp, & if cylinders = 6 \\
22.6749 - 0.2501 hp, & if cylinders = 8}$$

It has a substantially higher adjusted $R^2$ than the base model (*mpg \~ hosepower*). The output does indicate that not all levels in cylinders are particularly useful and that's due to the small number of points for odd-numbered cylinders and their similarity in performance with cars with an even number of cylinders. For completeness, let's see what this model looks like on a plot:

```{r}
ggplot(autompgclean, aes(x=horsepower, y=mpg, group = cylinders))+
  geom_point(aes(colour=factor(cylinders)))+
  stat_smooth(method=lm,formula = y ~ x, se=TRUE)+
  theme_bw()
```

**Question:** Can you interpret this plot? Talk to a colleague and/or a tutor.

Try changing **SE** to TRUE in the code above and comment on the uncertainty around the fit.

Now let's try to change the grouping for cylinders and reduce it to three levels $[3,4]$, $[5,6]$, $[7,8]$ and repeat the fit above:

```{r}
autowithcylgroups = autompgclean %>%
              mutate(cyl_group = cut(cylinders, c(3,4,6,8), include.lowest=TRUE))
head(autowithcylgroups)
```

```{r}
summary(lm(mpg ~ horsepower*factor(cyl_group), data = autowithcylgroups))
```

```{r}
ggplot(autowithcylgroups, aes(x=horsepower, y=mpg, group = cyl_group))+
  geom_point(aes(colour=factor(cyl_group)))+
  stat_smooth(method=lm,formula = y ~ x)+
  theme_bw()
```

**Exercise:** Reduce the grouping further by splitting the cars into two groups instead of three. Repeat the model fitting process above, write down the equations derived from the fit, and decide which model would be the most appropriate to use with respect to quality of fit and interpretability. Discuss this with your colleagues and/or a tutor.

```{r}
autowithcylgroups2 = autompgclean %>%
              mutate(cyl_group = cut(cylinders, c(3,6,8), include.lowest=TRUE))
head(autowithcylgroups2)
summary(lm(mpg ~ horsepower*factor(cyl_group), data = autowithcylgroups2))
ggplot(autowithcylgroups2, aes(x=horsepower, y=mpg, group = cyl_group))+
  geom_point(aes(colour=factor(cyl_group)))+
  stat_smooth(method=lm,formula = y ~ x)+
  theme_bw()
```

Now let's go back to the principles of piecewise regression. While we know that cylinders are a design choice in a car that directly impact horsepower and mpg, it could have been the case that the variable wasn't available or our understanding of how engines work is limited (that's why it is a good ideal to talk to an expert in the field when modelling their data).

We could cut the data with respect to *horsepower* and use piecewise regression. We will use the **cut** and **mutate** again but this time we will target *horsepower*. The minimum value taken by *horsepower* is 46 and the maximum is 230 which we include in the cut, with two other arbitrarily chosen points:

```{r}
autowithgroups = autompgclean %>%
              mutate(sub_group = cut(horsepower, c(46,100,180,230), include.lowest=TRUE))
ggplot(autowithgroups, aes(x=horsepower, y=mpg, group = sub_group))+
      geom_point()+
      stat_smooth(method=lm,formula = y ~ x)+
      theme_bw()
```

Since we have split the groups based on our feature, the regression lines seem to be roughly connected. We call the breakpoints we have chosen *knots*.

**Exercise:** Change the number of knots and their position. Comment on the quality of the model you have produced.

```{r}
summary(lm(mpg ~ horsepower*factor(sub_group), data = autowithgroups))
```

```{r}
autowithgroups = autompgclean %>%
              mutate(sub_group = cut(horsepower, c(46,100,150,230), include.lowest=TRUE))
ggplot(autowithgroups, aes(x=horsepower, y=mpg, group = sub_group))+
      geom_point()+
      stat_smooth(method=lm,formula = y ~ x)+
      theme_bw()
summary(lm(mpg ~ horsepower*factor(sub_group), data = autowithgroups))
```

**Polynomial regression**

In the previous section, we used line segments to model *mpg* as a function of *horsepower*. There is a clear non-linear trend on the data that we tried to mitigate for by using piecewise regression.

You have already seen briefly in Intro to Stats the use of the **poly** function to add polynomial terms to your regression. When you use the **poly** function, all powers of the given variable are added to the module up to the limit you specified. Let's try to model *mpg* as a second-order polynomial of *horsepower*:

```{r}
ggplot(autompgclean, aes(x=horsepower, y=mpg))+
  geom_point()+
  stat_smooth(method=lm,formula = y ~ poly(x,2))+
  theme_bw()
```

```{r}
summary(lm(mpg ~ poly(horsepower, 2), data = autompgclean))
```

While the model captures the underlying non-linear trend better than the base linear model, this isn't a particularly good model. You can try to improve it by tuning a piecewise polynomial regression or by introducing a factor like number of cylinders.

```{r}
autowithgroups = autompgclean %>%
              mutate(sub_group = cut(cylinders, c(3,5,6,8), include.lowest=TRUE))
ggplot(autowithgroups, aes(x=horsepower, y=mpg, group = sub_group))+
      geom_point()+
      stat_smooth(method=lm,formula = y ~ poly(x,2))+
      theme_bw()
```

```{r}
summary(lm(mpg ~ poly(horsepower,2)*sub_group, data = autowithgroups))
```

**Exercise:** Try implementing the same strategies you used for improving your model as in the previous section.

(a) model *mpg* as a function of a polynomial of *horsepower* and factors of *cylinders*. Split the data on cars in groups of fewer than 6 cylinders and 6 or more cylinders.

    ```{r}
    auto_model1 = autompgclean %>%
                  mutate(sub_group = cut(cylinders, c(3,6,8), include.lowest=TRUE))
    ggplot(auto_model1, aes(x=horsepower, y=mpg, group = sub_group))+
          geom_point(aes(colour=factor(sub_group)))+
          stat_smooth(method=lm,formula = y ~ poly(x,2))+
          theme_bw()
    ```

(b) model *mpg* as a function of a polynomial of *horsepower* with knots. Try to tune the location of the knots manually.

    ```{r}
    auto_model2 = autompgclean %>%
                  mutate(sub_group = cut(horsepower, c(46,100,120,150,180,230), include.lowest=TRUE))
    ggplot(auto_model2, aes(x=horsepower, y=mpg, group = sub_group))+
          geom_point(aes(colour=factor(sub_group)))+
          stat_smooth(method=lm,formula = y ~ poly(x,2))+
          theme_bw()
    ```

### **Splines**

Splines are functions that are defined by different polynomials on different parts of an interval. So similar to what you just did with piecewise polynomial regression but with a degree with smoothing to ensure the knots aren't as visible. They are useful for interpolation and smoothing of data, as well as for modeling complex curves and surfaces. Splines can have different degrees, such as linear, quadratic, or cubic, depending on the number of coefficients in each polynomial.

Splines impose smoothness conditions on the fitted curve, such as continuity and differentiability, while piecewise polynomial regression does not. This means that splines can produce smoother and more natural-looking curves than piecewise polynomial regression, which can have sharp changes or kinks at the boundaries of the subintervals as we have seen in the previous section.

Splines can use higher-degree polynomials, such as cubic or quartic, while piecewise polynomial regression usually uses linear or quadratic polynomials. This means that splines can capture more complex and flexible patterns in the data than piecewise polynomial regression, which can be too rigid or simplistic.

There are many different types of splines, the two most commonly used are:

-   natural splines: a cubic spline that has zero second derivatives at the endpoints of the interval of interpolation. This means that the natural spline does not curve beyond the boundary knots, but extrapolates linearly.

-   b-splines or basis splines: splines of any order forming a combination of flexible bands with continuity over the knots. They are zero outside the bounds of the knot boundaries. Every other type of spline can be seen as a b-spline.

We will use the package **splines** to fit a natural spline to our data:

```{r}
library(splines)
knots = c(100, 150)
sp_model = lm(mpg ~ ns(horsepower, knots = knots), data=autompgclean)
summary(sp_model) #ignore all information about coefficients, they aren't meant to be interpreted in a traditional way.
```

```{r}
ggplot(autompgclean, aes(x=horsepower, y=mpg))+
  geom_point()+
  stat_smooth(method=lm,formula = y ~ splines::ns(x, knots = knots))+
  theme_bw()
```

**Exercise:** Try changing the knot positions, can you find a better fitting model?

The quantiles of the model feature are normally used as the initial knots for fitting a spline. Instead of specifying the knot locations, you can give the pass the parameter *df* to the **ns** function. Say *df=3*, the knots will be placed on the first quartile, median, and third quartile of your variable:

```{r}
ggplot(autompgclean, aes(x=horsepower, y=mpg))+
  geom_point()+
  stat_smooth(method=lm,formula = y ~ splines::ns(x, df = 3))+
  theme_bw()
```

```{r}
quant_model = lm(mpg ~ ns(horsepower, df = 3), data=autompgclean)
summary(quant_model)
```

**Exercise:** Load the caret package and perform k-fold cross-validation in all your models. The procedure to perform 10-fold CV is below as a starting point.

Which model would you choose as your final model?

```{r}
library(caret)
ctrl_kfold = trainControl(method = "cv", number = 10)
lm_kfold = train(mpg ~ horsepower,
              data = autompgclean,
              method = "lm",
              metric = "RMSE",
              trControl = ctrl_kfold)
```

```{r}
lm_kfold
```

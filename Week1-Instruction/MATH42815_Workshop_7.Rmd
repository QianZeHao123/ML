#### MATH42815 - Machine Learning - 2023/24

## Machine Learning - Workshop 7

**Aim**: The aim of this workshop is to introduce decision trees and CART algorithms.

### **Decision Trees**

*''Rincewind thought: I can't be talking to a tree. If I was talking to a tree I'd be mad, and I'm not mad, so trees can't talk.''* - Terry Pratchett, The Light Fantastic

A decision tree is a tree-shaped graph used to visualize and model decision paths. They can be used to describe data, show probabilities, costs/rewards, etc.

Paddington has decided to open a marmalade sandwich shop. He makes his marmalade using Aunt Lucy's special recipe. Tesco wants to buy Aunt Lucy's recipe. If Paddington sells the recipe, he will be paid £800k. If he decides to carry on with his shop, he will be susceptible to market variations. If sales are high, he will make a profit of £2m. If sales are average, he will make £1m. If sales are poor, he will make £200k.

We can draw a decision tree to represent this problem and help us visualize Paddington's options:

<div>

<img src="https://raw.githubusercontent.com/ccscaiado/MLRepo/main/Paddington.png" width="400"/>

</div>

In this tree, the orange square is called a *decision node* where Paddington has to decide whether to sell or not the recipe. In CART, these will also be referred to as a **Split Point**. The green circle is a *chance node* and it shows multiple uncertain outcomes. The values at the end of each branck are called *endpoint nodes* or **Leaf Nodes**. Sometimes they are also called **Terminal Nodes**.

If profit is what matter's to Paddington, he'd need to carefully consider how likely each of these scenarios mapped to the *chance node* are before making a decision. Ideally, he would have run the shop for a few months and would be able to estimate profits or he might have access to a dataset of similar shops and their sales profiles.

Whatever process Paddington chooses to take, Classification and Regression Trees (CART) are likely to be useful.

### Classification and Regression Trees (CART)

CART is a generic term used to describe tree algorithms but it can also be used to refer to Breiman's CART algorithm. While this unifying term exists, there are two families of trees within this envelope:

-   **Classification Trees:** a decision tree used for classification and predicts a categorical response.

-   **Regression Trees:** a decision tree that is used to model numeric response variables using regression.

Trees will become forests later!

Stephanie and Tony at R2D3 created a very impressive introduction to tree models available [here](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/). The dataset they used can be found in [Kaggle](https://github.com/jadeyee/r2d3-part-1-data). It contains 492 observations over 8 variables. Let's load it and work through some of the steps they have taken:

```{r}
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
# current_directory
current_directory = getwd()
# read_csv
# joint file path
file = file.path(current_directory, "part_1_data.csv")
```

```{r}
library(dplyr)
houses = read.csv(file, header=TRUE)
head(houses)
dim(houses)
```

We plot the scatter of the price per square feet vs elevation and colour them according to the city where the property is located; blue for New York and green for San Francisco. We can see from the plot that properties in San Francisco are generally cheaper than New York and located at higher elevations

```{r}
library("ggplot2")
ggplot(houses, aes(x=price_per_sqft,y=elevation))+
  geom_point(aes(colour=factor(in_sf)))+
  xlab("Price per Square feet ($)")+
  ylab("Elevation (m)")+
  labs(colour="")+
  scale_color_manual(values=c("dodgerblue4","darkgreen"),labels=c("New York","San Francisco"))
```

```{r}
ggplot()+
  xlab("Price per Square feet ($)")+
  ylab("Elevation (m)")+
  labs(colour="")+
  scale_color_manual(values=c("dodgerblue4","darkgreen"),labels=c("New York","San Francisco"))+
  annotate("rect",
            xmin=max(subset(houses,in_sf==1)$price_per_sqft),
            xmax=max(houses$price_per_sqft),
            ymin=min(houses$elevation),
            ymax=max(subset(houses, in_sf==0)$elevation),
            fill="cornflowerblue", alpha = 0.2)+
    annotate("rect",
            xmin=min(houses$price_per_sqft),
            xmax=max(houses$price_per_sqft),
            ymin=max(subset(houses, in_sf==0)$elevation),
            ymax=max(houses$elevation),
            fill="chartreuse", alpha = 0.2)+
  geom_point(data=houses, aes(x=price_per_sqft,y=elevation,colour=factor(in_sf)))

```

We should look at the other variables and colour the pairs of plots according to their location to see if similar patterns exist elsewhere:

```{r}
library("GGally")
ggpairs(houses, aes(color=factor(in_sf), alpha = 0.5), columns=2:8)+
  scale_color_manual(values=c("darkgreen","dodgerblue4"))+
  scale_fill_manual(values=c("darkgreen","dodgerblue4"))+
  theme_bw()
```

**Question:** Which other split points can you see?

### Tradeoffs

Say that we use only one split (by elevation) and classify all properties with elevation below 73 metres as being in New York, and the remaining San Francisco:

```{r}
library(tidyr)
temp_houses = houses %>%
  mutate(city = case_when(in_sf == 1 ~ "SF",
                          in_sf == 0 ~ "NY")) %>%
  mutate(
    class_part = case_when(
      elevation < 73 ~ "NY (blue)",
      elevation >= 73 ~ "SF (green)",
      .default = "undefined"
    )
  )

temp_houses %>%
  group_by(city, class_part) %>%
  summarise(n = n()) %>%
  pivot_wider(names_from = city, values_from = n)
```

We can see from the table above that 222 properties were correctly classified as being in New York, 96 were correctly classified as being in San Francisco, 2 New York properties were misclassified as being in NY, and 172 properties from San Francisco were misplaced in NY.

Let's now try to implement the partitions we had with the rectangles we plotted previously. The blue rectangle can be written as [2240, 4601] x [0, 73], and the green rectangle as [270, 4601] x [73, 238]. We say that all properties in the blue rectangle are in New York, all properties in the green rectangle are in San Francisco, and the remaining can't be classified yet.

```{r}
temp_houses = houses %>%
  mutate(city = case_when(in_sf == 1 ~ "SF",
                          in_sf == 0 ~ "NY")) %>%
  mutate(
    class_part = case_when(
      price_per_sqft >= 2240 & elevation <= 73 ~ "NY (blue)",
      price_per_sqft <= 4601 &
        elevation >= 73 ~ "SF (green)",
      .default = "undefined"
    )
  )

temp_houses %>%
  group_by(city, class_part) %>%
  summarise(n = n()) %>%
  pivot_wider(names_from = city, values_from = n)
```

The table above shows that 350 properties weren't classified, 43 were correctly labelled as being in NY, 96 were correctly labelled as SF, and 3 were incorrectly labelled. While we have fewer classification errors, we also have a large proportion of the dataset unclassified.

We are ultimately looking for the best possible split given our training data. And how we define **best split** depends on the method we choose to calculate and value each split.

### Best Split

The CART algorithm as developed by Breiman uses the **Gini Impurity** (not the same as the Gini index used in economics but developed by the same person!) as the measure to place splits. Other tree algorithms such as ID3, C4.5 and C5.0 use **entropy-based** mesures.

#### **Gini Impurity**

The Gini Impurity is a measure that indicates the likelihood of a new data point being misclassified if it were given a random class label according to the class label distribution in the dataset.

The Gini Impurity ranges from 0 to 0.5; the index is 0 when the dataset is "pure" and only contains one class. Say all properties in the training data were located in New York, then, based on the training data, there would be a 0% chance of a new data point being incorrectly classified.

Say we had $K$ classes and the probabilities $P(i)$ for each class i, we write the Gini Impurity as:

$$
G(k) = \sum_{i=1}^K P(i)(1-P(i)) = 1 - \sum_{i=1}^K P(i)^2
$$

$$ G(k) = \sum_{i=1}^K P(i)(1-P(i)) = 1 - \sum_{i=1}^K P^2(i) $$

For example, if we had two classes (e.g. New York and San Francisco), and the probability associated to each of those classes were the same as the proportions in the original dataset. We have 224 properties in New York and 268 properties in San Francisco; total 492. So $P(NY) = 224/492 \approx 0.46$ and $P(SF)=268/492 \approx 0.54$. And the Gini impurity is given by

$$
G(2) = 1-P(NY)^2-P(SF)^2 = 0.496.
$$

This means that there is a 49.6% chance that a new data point will be incorrectly classified. So as we partition our dataset using different features, the aim is to reduce this value.

Let's use the *rpart* package to look for splits:

```{r}
housesCity = houses %>%
  mutate(city = case_when(
                          in_sf == 1 ~ "SF",
                          in_sf == 0 ~ "NY")) %>%
  # the variable city is easier to read, remove in_sf
  select(-in_sf)

library(ggparty)
library(rpart)
part_elevation = rpart(data = housesCity,
                       city ~ elevation,
                       method = "class",
                       maxdepth = 1)
part_elevation
```

Here we set the *method* to classification which will use the Gini impurity measure to find splits. We have also set *maxdepth* of the tree to 1 for illustration, that means that this tree will have only two layers.

In the output above, we see that one split was placed at elevation 30.5m. With this approach, 300 properties from the training data would be correctly classified as located in NY and 85 misclassified; 192 values would be correctly classified in SF and 9 misclassified.

The output of this function can get quite complicated to read as the number of splits and levels increase so, where possible, we should try to plot our tree. The plot below shows how many points belong to each node and how they were classified.

```{r}
plot(as.party(part_elevation))
```

Now let's remove the depth restriction in our model:

```{r}
part_elevation2 = rpart(data = housesCity,
                       city ~ elevation,
                       method = "class")
part_elevation2
plot(as.party(part_elevation2))
```

Two more splits were added. If the elevation is above or equal 30.5, the model will classifcy everything as being in San Francisco. If the elevation is below 30.5, the properties are split again at 7.5m. If the elevation is above 7.5m, around 80% of properties are in NY with a small number in SF. If the elevation is below 7.5, another split is added at 4.5, and so on.

This isn't necessarily a better model even if more properties are correctly classified. In principle, you could create a very large number of partitions that would eventually correctly allocate all observations but the predictive power, explainability, and complexity of the final model would not be adequate.

The **printcp** function can be applied to a tree model produced by **rpart**. It returns the the **cost complexity** parameter (CP) for each step of the construction; the cost is the number of leaf nodes in the tree multiplied by CP plus the fraction of misclassified cases.

The table in the output below shows us three models. The first one is the base model without any splits. The second is a tree with one split (nsplit = 1), and cross-validation error 0.43304. The third is the tree displayed above with nsplit = 3, a lower relative error (fewer misclassifications) but a higher cross-validation error 0.44643.

As is the case with all other models we have seen so far, an overly specified model that fits data well doesn't necessarily work well for prediction, and cross-validation is a way of assessing how well a model can be generalized. In this problem, model 1 doesn't have the lowest relative error but it does have the lowest cross-validated error indicating that it is likely to be the best if we were to use it for prediction.

```{r}
printcp(part_elevation2)
```

We will then **prune** our tree using the lowest cross-validate error as the model selection criteria. We need to retrieve the CP value that corresponds to the lowest xerror. See below two different ways of getting the relevant value:

```{r}
cp = part_elevation2$cptable %>%
                      as.data.frame %>%
                      slice(which.min(xerror)) %>%
                      select(CP) %>%
                      as.numeric
```

```{r}
#opt = which.min(part_elevation2$cptable[,"xerror"])
#cp = part_elevation2$cptable[opt, "CP"]
pruned_model = prune(part_elevation2,cp)
plot(as.party(pruned_model))
```

**Exercise:** Before moving to the next model, try the process above with other variables. Start with *city \~ price_per_sqft*. What do the best models with a single feature look like? Which feature seems to be the best at splitting this dataset?

Let's add more variables to our tree and see what happens:

```{r}
part_house = rpart(data = housesCity,
                       city ~ .,
                       method = "class")
part_house
```

```{r}
plot(as.party(part_house))
```

```{r}
printcp(part_house)
```

It seems that the full model is the model with the lowest cross-validated error so pruning isn't required.

The **ggparty** we loaded above comes with tools to draw trees using *ggplot*. We have been using it primarily to convert the *rpart* objects into **party**s! If you don't like how the trees above look, you can use the **autoplot** function or build your own with **ggpart**. You can find examples [here](https://cran.r-project.org/web/packages/ggparty/vignettes/ggparty-graphic-partying.html), for now we will apply autoplot to \*part_house\*\* to see a slightly tidier model.

```{r}
autoplot(as.party(part_house))
```

### Regression Trees

Regression tree models are very similar to the classification models above. Instead of a categorical variable, we model a continuous variable. The training data is recursively split into smaller subsets and the objective is to minimize residues. The model returns the mean of the response variable values for that subset. The splitting criteria is normally based on the RMSE; the split that leads to the greatest reduction in residual error is selected.

Say we want to build a model to predict the price of a property. We will use the other variables in the **housesCity** dataset as features for our model. Let's start with a simple model and try to model *price* as a function of *elevation* using a regression tree.

```{r}
part_prices = rpart(data = housesCity,
                    price ~ elevation
                    )
part_prices
```

```{r}
plot(as.party(part_prices))
```

And now to do some pruning. We use the same process as we used for the classification models:

```{r}
printcp(part_prices)
```

```{r}
cp = part_prices$cptable %>%
                      as.data.frame %>%
                      slice(which.min(xerror)) %>%
                      select(CP) %>%
                      as.numeric
```

The final tree model for *price_per_sqft \~ elevation* does the following:

1.  split the training data into two subsets. Take all the points for which elevation is greater of equal than 23.5 and calculate the mean of *price*. The remaining points are used for the next split.

2.  take all observations for which the elevation is less than 14.5m and return the mean *price_per_sqft*.

3.  Split the remaining points into two further sets at elevation = 15.5m and return their means.

```{r}
pruned_prices = prune(part_prices, cp)
plot(as.party(pruned_prices))
```

Let's try to use this model for prediction. Create a dummy testing dataset and use the **predict** function to see how it behaves:

```{r}
new_houses = data.frame(elevation = c(5, 10, 15, 20, 25))
new_houses %>% mutate(pred_prices = predict(pruned_prices, newdata = new_houses))
```

**Exercise:** Now add the other features available to your model and return a regression tree that can be used to estimate *price*. Make sure to prune your tree if necessary and test it with a small sample.

### Where is my caret?

To no one's surprise, **caret** works with **rpart**, but the "best" model returned won't necessarily be the same as the ones we have seen above. Caret does use a better optimization process with a finer grid to find CP and we have better control over the cross-validation process.

```{r}
library(caret)
fitControl = trainControl(method = "cv",number = 10)
rpartFit = train(city ~ .,
            data = housesCity,
            method = "rpart",
            trControl = fitControl)
plot(as.party(rpartFit$finalModel))
```

```{r}
library(caret)
fitControl = trainControl(method = "cv",number = 10)
rpartFit = train(price ~ .,
            data = housesCity,
            method = "rpart",
            trControl = fitControl)
plot(as.party(rpartFit$finalModel))
```

# read_csv
# joint file path
file = file.path(current_directory, "part_1_data.csv")
houses = read.csv(file, header = TRUE)
head(houses)
training
# Caret implements standardization where relevation or you can specify it during pre-processing
range_fun = function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
houses$price_scale = range_fun(houses$price)
houses$elevation_scale = range_fun(houses$elevation)
houses$sqft_scale = range_fun(houses$sqft)
set.seed(2024)
ind = sample(1:nrow(houses), 350) #about 75% for training
training = houses[ind, ]
testing = houses[-ind, ]
train = as.matrix(training[, c("price_scale", "elevation_scale", "sqft_scale")])
target = training$in_sf
training
train
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
x = c(1, 1, 1, 1) # all criteria met, change according to scenario
w = c(0.5, 0.8, 0.2, 0.9) # weights - fixed for now
bias = -2 #threshold
activation = function(x, w, bias){
v = x %*% w  # dot product of x and w
decision = ifelse(v+bias > 0, 1, 0)
return(as.numeric(decision))
}
print(activation(x, w, bias))
all_x = as.matrix(expand.grid(0:1, 0:1, 0:1, 0:1))
w = c(0.5, 0.8, 0.2, 0.9) # weights - fixed for now
bias = -2 #threshold
results = activation(all_x, w, bias)
df = data.frame(cbind(all_x, results))
df
percep_training = function(train,
target,
weight,
bias,
learning,
epochs = 10) {
epoch = 0
n_train = nrow(train)
while (TRUE) {
if (epoch > epochs) {
print(c("Training complete - too many epochs", epochs))
print(c("Weights:", weight))
print(c("Bias:", bias))
return(list(weight, bias)) # we are done, return weights and bias
}
count_errors = 0
for (i in 1:n_train) {
x_input = train[i, ]
x_target = target[i]
guess = activation(x_input, weight, bias)
error = x_target - guess
#if the perceptron guessed correctly, the error is 0.
#If the perceptron guessed incorrectly, the error is -1 or 1 and that will be direction in which the weights will be updated.
if (error != 0) {
count_errors = count_errors + 1
weight = weight + learning * error * x_input
bias = bias + learning * error
}
if (i == n_train) {
epoch = epoch + 1
}
}
if (count_errors == 0) {
print("Training complete - linearly separable")
print(c("Weights:", weight))
print(c("Bias:", bias))
return(list(weight, bias)) # we are done, return weights and bias
}
}
}
train_AND = matrix(c(0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1),
ncol = 3,
byrow = TRUE)
train_AND
w = c(0.5, 0.5)
bias = 0
learning_rate = 0.1
ttAND = percep_training(train_AND[, 1:2], train_AND[, 3], w, bias, learning_rate)
activation(train_AND[, 1:2], ttAND[[1]], ttAND[[2]])
# if you have studied circuits or comnational logic before, this will likely make sense. If you didn't, don't worry, I'm creating a LS dataset.
circuit = as.data.frame(expand.grid(0:1, 0:1, 0:1, 0:1))
TF = (circuit == 1)
circ_out = as.numeric(((TF[, 1] & TF[, 2]) | ((TF[, 3] | TF[, 4]))))
circuit$out = circ_out
circ_out
all_episodes = circuit
colnames(all_episodes) = c("X1", "X2", "X3", "X4", "D")
head(all_episodes)
train = as.matrix(all_episodes[, 1:4])
target = all_episodes$D
w = c(0.5, 0.8, 0.2, 0.9) #you can replace this with a random vector, use runif(4)
bias = 0
learning_rate = 0.01
epoch_num = 100
ttSquid = percep_training(train, target, w, bias, learning_rate, epoch_num)
all_episodes$pred = activation(train, ttSquid[[1]], ttSquid[[2]])
#Build a confusion matrix and check how many are misclassified
all_episodes$D
all_episodes$pred
table(all_episodes$D, all_episodes$pred)
circuit8 = as.data.frame(expand.grid(0:1, 0:1, 0:1, 0:1, 0:1, 0:1, 0:1, 0:1))
TF = (circuit8 == 1)
circ_out = as.numeric(((TF[, 1] &
TF[, 2]) |
((TF[, 3] | TF[, 4]))) & ((TF[, 5] &
(TF[, 6])) & (TF[, 7] | TF[, 8])))
circuit8$out = circ_out
# remove for NCC
train = as.matrix(circuit8[,1:8])
target = circ_out
w = runif(8)
bias = 0
learning_rate = 0.01
epoch_num = 1000
tt8 = percep_training(train, target, w, bias, learning_rate, epoch_num)
circuit8$pred = activation(train, tt8[[1]], tt8[[2]])
#Build a confusion matrix and check how many are misclassified
head(circuit8)
table(circuit8$out, circuit8$pred)
library(dplyr)
# current directory
current_directory = getwd()
# read_csv
# joint file path
file = file.path(current_directory, "part_1_data.csv")
houses = read.csv(file, header = TRUE)
head(houses)
# Caret implements standardization where relevation or you can specify it during pre-processing
range_fun = function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
houses$price_scale = range_fun(houses$price)
houses$elevation_scale = range_fun(houses$elevation)
houses$sqft_scale = range_fun(houses$sqft)
set.seed(2024)
ind = sample(1:nrow(houses), 350) #about 75% for training
training = houses[ind, ]
testing = houses[-ind, ]
train = as.matrix(training[, c("price_scale", "elevation_scale", "sqft_scale")])
target = training$in_sf
weight = c(0.5,0.5,0.5)
bias = 0
learning = 0.01
epochs = 100
Perc_sf = percep_training(train, target, weight, bias, learning, epochs)
predSF = activation(train, Perc_sf[[1]], Perc_sf[[2]])
table(predSF, target)
testSF = activation(as.matrix(testing[, c("price_scale", "elevation_scale", "sqft_scale")]), Perc_sf[[1]], Perc_sf[[2]])
table(testSF, testing$in_sf)
library(randomForest)
set.seed(2020)
rf = randomForest(factor(in_sf) ~ price_scale + elevation_scale + sqft_scale, data = training)
rf
predRF = predict(rf, newdata = testing)
table(predRF, testing$in_sf)
sigmoid_activation = function(x, w, b){
v = x %*% w + b  # dot product of x and w
sig = 1/(1+exp(-v))
return(as.numeric(sig))
}
sig_squared_loss = function (x, y, w, b){
return(sum(0.5*(sigmoid_activation(x, w, b) - y)^2))
}
grad_w = function (w, b, x, y){
y_pred = sigmoid_activation(x, w, b)
delta_w = (y_pred - y)*y_pred*(1-y_pred)*x
return(delta_w)
}
grad_b = function(w, b, x, y){
y_pred = sigmoid_activation(x, w, b)
delta_b = (y_pred - y)*y_pred*(1-y_pred)
return(delta_b)
}
grad_desc_sigmoid = function (train, target, weight, bias, learning, epochs = 10){
for (i in 1:epochs){
dw = colSums(grad_w(weight, bias, train, target))
db = sum(grad_b(weight, bias, train, target))
weight = weight - learning*dw
bias = bias - learning*db
}
return(list(weight,bias))
}
weight = c(0.5,0.5,0.5)
bias = 0
learning = 0.001
epochs = 10000
GDS = grad_desc_sigmoid(train, target, weight, bias, learning, epochs)
probSF = sigmoid_activation(train, GDS[[1]], GDS[[2]])
predSF = as.numeric(probSF>0.5)
table(predSF, target)
probtestGDS = sigmoid_activation(as.matrix(testing[,c("price_scale","elevation_scale","sqft_scale")]), GDS[[1]], GDS[[2]])
testGDS = as.numeric(probtestGDS>0.5)
table(testGDS, testing$in_sf)
quality_measures = function(pred, actual) {
confusion = table(pred, actual)
accuracy = sum(diag(confusion)) / sum(confusion) #accuracy - proportion of correctly classified
prec = confusion[2, 2] / sum(confusion[, 2]) # precision - positive predictive value
sens = confusion[2, 2] / sum(confusion[2, ]) # sensitivity - true positive rate
spec = confusion[1, 1] / sum(confusion[1, ]) # specificity - true negative rate
fscore = 2 * (sens * prec) / (sens + prec) #F-score - measure of predictive performance
print(c("Precision: ", prec))
print(c("Sensitivity: ", sens))
print(c("Specificity: ", spec))
print(c("F Score: ", fscore))
print(c("Accuracy: ", accuracy))
print(confusion)
}
### Perceptron
quality_measures(testSF, testing$in_sf)
### Random Forest
quality_measures(predRF, testing$in_sf)
### Sigmoid
quality_measures(testGDS, testing$in_sf)
train
library(torch)
# Assuming you have data frames 'train_df' and 'test_df'
train_tensor <- tensor(as.matrix(train_df), dtype = torch_float32())
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
library(torch)
library(torch)
circuit8 = as.data.frame(expand.grid(0:1, 0:1, 0:1, 0:1, 0:1, 0:1, 0:1, 0:1))
TF = (circuit8 == 1)
circ_out = as.numeric(((TF[,1] & TF[,2]) | ((TF[,3] | TF[,4]))) & ((TF[,5] & TF[,6]) & (TF[,7] | TF[,8])))
circuit8$out = circ_out
head(circuit8)
torch_train = torch_tensor(    #use torch_tensor to convert your data
as.matrix(circuit8[,1:8]), #take the first 8 columns containing the features we will use for training
#and convert the data frame to a matrix using as.matrix
dtype = torch_float())     #choose the type of data in your matrix, it is almost always a float
torch_target = torch_tensor(
as.numeric(circuit8[,9]),  #take the last column with the target response values and ensure they are numeric
dtype = torch_float())
criterion = nn_bce_loss() #binary cross-entropy (bce)
set.seed(42)
model = nn_sequential(
nn_linear(8,1),
nn_sigmoid()
)
optimizer = optim_adam(model$parameters, lr = 0.01) #gradient descent on steroids with a learning rate of 0.01
epochs = 2000 #many epochs
for (i in 1:epochs) {
optimizer$zero_grad()  #we don't want the gradients to be accummulated but reset in each epoch, there'll be cases where this will be needed
y_pred = model(torch_train) #fit the current model, return predicted values
loss = criterion(y_pred, torch_target) #calculate loss
loss$backward() #backward propagate your losses
optimizer$step()    #next step
if (i %% 100 == 0) {  #some verbose to keep track of how well your neural net is being trained
corrects <- (as.numeric(y_pred > 0.5) == torch_target)
accuracy <- corrects$sum()$item()/torch_target$size()
cat("Epoch:", i, "Loss:", loss$item(), "Accuracy:", accuracy, "\n")
}
}
quality_measures = function(pred, actual){
confusion = table(pred, actual)
accuracy = sum(diag(confusion))/sum(confusion) #accuracy - proportion of correctly classified
prec = confusion[2,2]/sum(confusion[,2]) # precision - positive predictive value
sens = confusion[2,2]/sum(confusion[2,]) # sensitivity - true positive rate
spec = confusion[1,1]/sum(confusion[1,]) # specificity - true negative rate
fscore = 2*(sens*prec)/(sens + prec) #F-score - measure of predictive performance
print(c("Precision: ", prec))
print(c("Sensitivity: ", sens))
print(c("Specificity: ", spec))
print(c("F Score: ", fscore))
print(c("Accuracy: ", accuracy))
print(confusion)
}
pred_values = as.numeric(model(torch_train)>0.5)
quality_measures(pred_values, as.numeric(torch_target))
set.seed(42)
model_3layer = nn_sequential(
#Input layer
nn_linear(8,4), #we choose to have 4 neurons in our hidden layer so the input layer needs to return 4 outputs
#Hidden layer
nn_linear(4,1), #4 neurons combined linearly to output one value
#Output layer
nn_sigmoid()
)
optimizer = optim_adam(model_3layer$parameters, lr = 0.01) #gradient descent on steroids with a learning rate of 0.01
epochs = 1000 #many epochs
for (i in 1:epochs) {
optimizer$zero_grad()  #we don't want the gradients to be accummulated but reset in each epoch, there'll be cases where this will be needed
y_pred = model_3layer(torch_train) #fit the current model, return predicted values
loss = criterion(y_pred, torch_target) #calculate loss
loss$backward() #backward propagate your losses
optimizer$step()    #next step
if (i %% 100 == 0) {  #some verbose to keep track of how well your neural net is being trained
corrects = (as.numeric(y_pred > 0.5) == torch_target)
accuracy = corrects$sum()$item()/torch_target$size()
cat("Epoch:", i, "Loss:", loss$item(), "Accuracy:", accuracy, "\n")
}
}
pred_values = as.numeric(model_3layer(torch_train)>0.5)
quality_measures(pred_values, as.numeric(torch_target))
model_3layer$parameters
library(dplyr)
file = "part_1_data.csv"
houses = read.csv(file,header=TRUE)
head(houses)
library(scales)
houses$price_scale = rescale(houses$price)
houses$elevation_scale = rescale(houses$elevation)
houses$sqft_scale = rescale(houses$sqft)
head(houses)
set.seed(2024)
ind = sample(1:nrow(houses), 350) #about 75% for training
training = houses[ind,]
testing = houses[-ind,]
train = as.matrix(training[,c("price_scale","elevation_scale","sqft_scale")])
target = as.matrix(training$in_sf)
criterion = nn_bce_with_logits_loss()
tensor_train = torch_tensor(train, dtype=torch_float())
tensor_target = torch_tensor(target, dtype=torch_float())
model_SF = nn_sequential(
#Layer 1
nn_linear(3,2),  #we have 3 features and we will add 2 neurons to our hidden layer
nn_relu(),       #the reLU or rectified linear unit function
#Layer 2
nn_linear(2,1)   #takes the output of layer 1 and linearizes it ready for the sigmoid in the nn_bce_with_logits_loss
#Layer 3 - is built-in our criteria this time
)
pred_temp = model_SF(tensor_train)
# Define cost and optimizer
criterion = nn_bce_with_logits_loss()
optimizer = optim_adam(model_SF$parameters, lr = 0.01)
epochs = 2000
# Train the net
for(i in 1:epochs){
optimizer$zero_grad()
y_pred = model_SF(tensor_train)
y_pred_class = (y_pred>0.5)
loss = criterion(y_pred, tensor_target)
loss$backward()
optimizer$step()
# Check Training
if(i %% 100 == 0){
corrects = (y_pred_class == tensor_target)
accuracy = corrects$sum()$item() / tensor_target$size(1)
cat(" Epoch:", i,"Loss: ", loss$item()," Accuracy:",accuracy,"\n")
}
}
y_pred = as.numeric(model_SF(tensor_train)>0.5)
quality_measures(y_pred, target)
test = as.matrix(testing[,c("price_scale","elevation_scale","sqft_scale")])
tensor_test = torch_tensor(test, dtype=torch_float())
y_test = as.numeric(model_SF(tensor_test)>0.5)
quality_measures(y_test, testing$in_sf)
library(randomForest)
set.seed(2020)
rf = randomForest(factor(in_sf) ~ price_scale + elevation_scale + sqft_scale, data = training)
rf
predRF = predict(rf, newdata = testing)
quality_measures(predRF, testing$in_sf)
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
library(torch)
circuit8 = as.data.frame(expand.grid(0:1, 0:1, 0:1, 0:1, 0:1, 0:1, 0:1, 0:1))
TF = (circuit8 == 1)
circ_out = as.numeric(((TF[,1] & TF[,2]) | ((TF[,3] | TF[,4]))) & ((TF[,5] & TF[,6]) & (TF[,7] | TF[,8])))
circuit8$out = circ_out
head(circuit8)
torch_train = torch_tensor(    #use torch_tensor to convert your data
as.matrix(circuit8[,1:8]), #take the first 8 columns containing the features we will use for training
#and convert the data frame to a matrix using as.matrix
dtype = torch_float())     #choose the type of data in your matrix, it is almost always a float
torch_target = torch_tensor(
as.numeric(circuit8[,9]),  #take the last column with the target response values and ensure they are numeric
dtype = torch_float())
criterion = nn_bce_loss() #binary cross-entropy (bce)
set.seed(42)
model = nn_sequential(
nn_linear(8,1),
nn_sigmoid()
)
optimizer = optim_adam(model$parameters, lr = 0.01) #gradient descent on steroids with a learning rate of 0.01
epochs = 2000 #many epochs
for (i in 1:epochs) {
optimizer$zero_grad()  #we don't want the gradients to be accummulated but reset in each epoch, there'll be cases where this will be needed
y_pred = model(torch_train) #fit the current model, return predicted values
loss = criterion(y_pred, torch_target) #calculate loss
loss$backward() #backward propagate your losses
optimizer$step()    #next step
if (i %% 100 == 0) {  #some verbose to keep track of how well your neural net is being trained
corrects <- (as.numeric(y_pred > 0.5) == torch_target)
accuracy <- corrects$sum()$item()/torch_target$size()
cat("Epoch:", i, "Loss:", loss$item(), "Accuracy:", accuracy, "\n")
}
}
quality_measures = function(pred, actual){
confusion = table(pred, actual)
accuracy = sum(diag(confusion))/sum(confusion) #accuracy - proportion of correctly classified
prec = confusion[2,2]/sum(confusion[,2]) # precision - positive predictive value
sens = confusion[2,2]/sum(confusion[2,]) # sensitivity - true positive rate
spec = confusion[1,1]/sum(confusion[1,]) # specificity - true negative rate
fscore = 2*(sens*prec)/(sens + prec) #F-score - measure of predictive performance
print(c("Precision: ", prec))
print(c("Sensitivity: ", sens))
print(c("Specificity: ", spec))
print(c("F Score: ", fscore))
print(c("Accuracy: ", accuracy))
print(confusion)
}
pred_values = as.numeric(model(torch_train)>0.5)
quality_measures(pred_values, as.numeric(torch_target))
set.seed(42)
model_3layer = nn_sequential(
#Input layer
nn_linear(8,4), #we choose to have 4 neurons in our hidden layer so the input layer needs to return 4 outputs
#Hidden layer
nn_linear(4,1), #4 neurons combined linearly to output one value
#Output layer
nn_sigmoid()
)
optimizer = optim_adam(model_3layer$parameters, lr = 0.01) #gradient descent on steroids with a learning rate of 0.01
epochs = 1000 #many epochs
for (i in 1:epochs) {
optimizer$zero_grad()  #we don't want the gradients to be accummulated but reset in each epoch, there'll be cases where this will be needed
y_pred = model_3layer(torch_train) #fit the current model, return predicted values
loss = criterion(y_pred, torch_target) #calculate loss
loss$backward() #backward propagate your losses
optimizer$step()    #next step
if (i %% 100 == 0) {  #some verbose to keep track of how well your neural net is being trained
corrects = (as.numeric(y_pred > 0.5) == torch_target)
accuracy = corrects$sum()$item()/torch_target$size()
cat("Epoch:", i, "Loss:", loss$item(), "Accuracy:", accuracy, "\n")
}
}
pred_values = as.numeric(model_3layer(torch_train)>0.5)
quality_measures(pred_values, as.numeric(torch_target))
model_3layer$parameters
library(dplyr)
file = "part_1_data.csv"
houses = read.csv(file,header=TRUE)
head(houses)
library(scales)
houses$price_scale = rescale(houses$price)
houses$elevation_scale = rescale(houses$elevation)
houses$sqft_scale = rescale(houses$sqft)
head(houses)
set.seed(2024)
ind = sample(1:nrow(houses), 350) #about 75% for training
training = houses[ind,]
testing = houses[-ind,]
train = as.matrix(training[,c("price_scale","elevation_scale","sqft_scale")])
target = as.matrix(training$in_sf)
criterion = nn_bce_with_logits_loss()
tensor_train = torch_tensor(train, dtype=torch_float())
tensor_target = torch_tensor(target, dtype=torch_float())
model_SF = nn_sequential(
#Layer 1
nn_linear(3,2),  #we have 3 features and we will add 2 neurons to our hidden layer
nn_relu(),       #the reLU or rectified linear unit function
#Layer 2
nn_linear(2,1)   #takes the output of layer 1 and linearizes it ready for the sigmoid in the nn_bce_with_logits_loss
#Layer 3 - is built-in our criteria this time
)
pred_temp = model_SF(tensor_train)
# Define cost and optimizer
criterion = nn_bce_with_logits_loss()
optimizer = optim_adam(model_SF$parameters, lr = 0.01)
epochs = 2000
# Train the net
for(i in 1:epochs){
optimizer$zero_grad()
y_pred = model_SF(tensor_train)
y_pred_class = (y_pred>0.5)
loss = criterion(y_pred, tensor_target)
loss$backward()
optimizer$step()
# Check Training
if(i %% 100 == 0){
corrects = (y_pred_class == tensor_target)
accuracy = corrects$sum()$item() / tensor_target$size(1)
cat(" Epoch:", i,"Loss: ", loss$item()," Accuracy:",accuracy,"\n")
}
}
y_pred = as.numeric(model_SF(tensor_train)>0.5)
quality_measures(y_pred, target)
test = as.matrix(testing[,c("price_scale","elevation_scale","sqft_scale")])
tensor_test = torch_tensor(test, dtype=torch_float())
y_test = as.numeric(model_SF(tensor_test)>0.5)
quality_measures(y_test, testing$in_sf)
library(randomForest)
set.seed(2020)
rf = randomForest(factor(in_sf) ~ price_scale + elevation_scale + sqft_scale, data = training)
rf
predRF = predict(rf, newdata = testing)
quality_measures(predRF, testing$in_sf)

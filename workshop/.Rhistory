head(circuit8)
table(circuit8$out, circuit8$pred)
library(dplyr)
# current directory
current_directory = getwd()
# read_csv
# joint file path
file = file.path(current_directory, "part_1_data.csv")
houses = read.csv(file, header = TRUE)
head(houses)
# Caret implements standardization where relevation or you can specify it during pre-processing
range_fun = function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
houses$price_scale = range_fun(houses$price)
houses$elevation_scale = range_fun(houses$elevation)
houses$sqft_scale = range_fun(houses$sqft)
set.seed(2024)
ind = sample(1:nrow(houses), 350) #about 75% for training
training = houses[ind, ]
testing = houses[-ind, ]
train = as.matrix(training[, c("price_scale", "elevation_scale", "sqft_scale")])
target = training$in_sf
weight = c(0.5,0.5,0.5)
bias = 0
learning = 0.01
epochs = 100
Perc_sf = percep_training(train, target, weight, bias, learning, epochs)
predSF = activation(train, Perc_sf[[1]], Perc_sf[[2]])
table(predSF, target)
testSF = activation(as.matrix(testing[, c("price_scale", "elevation_scale", "sqft_scale")]), Perc_sf[[1]], Perc_sf[[2]])
table(testSF, testing$in_sf)
library(randomForest)
set.seed(2020)
rf = randomForest(factor(in_sf) ~ price_scale + elevation_scale + sqft_scale, data = training)
rf
predRF = predict(rf, newdata = testing)
table(predRF, testing$in_sf)
sigmoid_activation = function(x, w, b){
v = x %*% w + b  # dot product of x and w
sig = 1/(1+exp(-v))
return(as.numeric(sig))
}
sig_squared_loss = function (x, y, w, b){
return(sum(0.5*(sigmoid_activation(x, w, b) - y)^2))
}
grad_w = function (w, b, x, y){
y_pred = sigmoid_activation(x, w, b)
delta_w = (y_pred - y)*y_pred*(1-y_pred)*x
return(delta_w)
}
grad_b = function(w, b, x, y){
y_pred = sigmoid_activation(x, w, b)
delta_b = (y_pred - y)*y_pred*(1-y_pred)
return(delta_b)
}
grad_desc_sigmoid = function (train, target, weight, bias, learning, epochs = 10){
for (i in 1:epochs){
dw = colSums(grad_w(weight, bias, train, target))
db = sum(grad_b(weight, bias, train, target))
weight = weight - learning*dw
bias = bias - learning*db
}
return(list(weight,bias))
}
weight = c(0.5,0.5,0.5)
bias = 0
learning = 0.001
epochs = 10000
GDS = grad_desc_sigmoid(train, target, weight, bias, learning, epochs)
probSF = sigmoid_activation(train, GDS[[1]], GDS[[2]])
predSF = as.numeric(probSF>0.5)
table(predSF, target)
probtestGDS = sigmoid_activation(as.matrix(testing[,c("price_scale","elevation_scale","sqft_scale")]), GDS[[1]], GDS[[2]])
testGDS = as.numeric(probtestGDS>0.5)
table(testGDS, testing$in_sf)
quality_measures = function(pred, actual) {
confusion = table(pred, actual)
accuracy = sum(diag(confusion)) / sum(confusion) #accuracy - proportion of correctly classified
prec = confusion[2, 2] / sum(confusion[, 2]) # precision - positive predictive value
sens = confusion[2, 2] / sum(confusion[2, ]) # sensitivity - true positive rate
spec = confusion[1, 1] / sum(confusion[1, ]) # specificity - true negative rate
fscore = 2 * (sens * prec) / (sens + prec) #F-score - measure of predictive performance
print(c("Precision: ", prec))
print(c("Sensitivity: ", sens))
print(c("Specificity: ", spec))
print(c("F Score: ", fscore))
print(c("Accuracy: ", accuracy))
print(confusion)
}
### Perceptron
quality_measures(testSF, testing$in_sf)
### Random Forest
quality_measures(predRF, testing$in_sf)
### Sigmoid
quality_measures(testGDS, testing$in_sf)
train
library(torch)
# Assuming you have data frames 'train_df' and 'test_df'
train_tensor <- tensor(as.matrix(train_df), dtype = torch_float32())
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
library(torch)
library(torch)
circuit8 = as.data.frame(expand.grid(0:1, 0:1, 0:1, 0:1, 0:1, 0:1, 0:1, 0:1))
TF = (circuit8 == 1)
circ_out = as.numeric(((TF[,1] & TF[,2]) | ((TF[,3] | TF[,4]))) & ((TF[,5] & TF[,6]) & (TF[,7] | TF[,8])))
circuit8$out = circ_out
head(circuit8)
torch_train = torch_tensor(    #use torch_tensor to convert your data
as.matrix(circuit8[,1:8]), #take the first 8 columns containing the features we will use for training
#and convert the data frame to a matrix using as.matrix
dtype = torch_float())     #choose the type of data in your matrix, it is almost always a float
torch_target = torch_tensor(
as.numeric(circuit8[,9]),  #take the last column with the target response values and ensure they are numeric
dtype = torch_float())
criterion = nn_bce_loss() #binary cross-entropy (bce)
set.seed(42)
model = nn_sequential(
nn_linear(8,1),
nn_sigmoid()
)
optimizer = optim_adam(model$parameters, lr = 0.01) #gradient descent on steroids with a learning rate of 0.01
epochs = 2000 #many epochs
for (i in 1:epochs) {
optimizer$zero_grad()  #we don't want the gradients to be accummulated but reset in each epoch, there'll be cases where this will be needed
y_pred = model(torch_train) #fit the current model, return predicted values
loss = criterion(y_pred, torch_target) #calculate loss
loss$backward() #backward propagate your losses
optimizer$step()    #next step
if (i %% 100 == 0) {  #some verbose to keep track of how well your neural net is being trained
corrects <- (as.numeric(y_pred > 0.5) == torch_target)
accuracy <- corrects$sum()$item()/torch_target$size()
cat("Epoch:", i, "Loss:", loss$item(), "Accuracy:", accuracy, "\n")
}
}
quality_measures = function(pred, actual){
confusion = table(pred, actual)
accuracy = sum(diag(confusion))/sum(confusion) #accuracy - proportion of correctly classified
prec = confusion[2,2]/sum(confusion[,2]) # precision - positive predictive value
sens = confusion[2,2]/sum(confusion[2,]) # sensitivity - true positive rate
spec = confusion[1,1]/sum(confusion[1,]) # specificity - true negative rate
fscore = 2*(sens*prec)/(sens + prec) #F-score - measure of predictive performance
print(c("Precision: ", prec))
print(c("Sensitivity: ", sens))
print(c("Specificity: ", spec))
print(c("F Score: ", fscore))
print(c("Accuracy: ", accuracy))
print(confusion)
}
pred_values = as.numeric(model(torch_train)>0.5)
quality_measures(pred_values, as.numeric(torch_target))
set.seed(42)
model_3layer = nn_sequential(
#Input layer
nn_linear(8,4), #we choose to have 4 neurons in our hidden layer so the input layer needs to return 4 outputs
#Hidden layer
nn_linear(4,1), #4 neurons combined linearly to output one value
#Output layer
nn_sigmoid()
)
optimizer = optim_adam(model_3layer$parameters, lr = 0.01) #gradient descent on steroids with a learning rate of 0.01
epochs = 1000 #many epochs
for (i in 1:epochs) {
optimizer$zero_grad()  #we don't want the gradients to be accummulated but reset in each epoch, there'll be cases where this will be needed
y_pred = model_3layer(torch_train) #fit the current model, return predicted values
loss = criterion(y_pred, torch_target) #calculate loss
loss$backward() #backward propagate your losses
optimizer$step()    #next step
if (i %% 100 == 0) {  #some verbose to keep track of how well your neural net is being trained
corrects = (as.numeric(y_pred > 0.5) == torch_target)
accuracy = corrects$sum()$item()/torch_target$size()
cat("Epoch:", i, "Loss:", loss$item(), "Accuracy:", accuracy, "\n")
}
}
pred_values = as.numeric(model_3layer(torch_train)>0.5)
quality_measures(pred_values, as.numeric(torch_target))
model_3layer$parameters
library(dplyr)
file = "part_1_data.csv"
houses = read.csv(file,header=TRUE)
head(houses)
library(scales)
houses$price_scale = rescale(houses$price)
houses$elevation_scale = rescale(houses$elevation)
houses$sqft_scale = rescale(houses$sqft)
head(houses)
set.seed(2024)
ind = sample(1:nrow(houses), 350) #about 75% for training
training = houses[ind,]
testing = houses[-ind,]
train = as.matrix(training[,c("price_scale","elevation_scale","sqft_scale")])
target = as.matrix(training$in_sf)
criterion = nn_bce_with_logits_loss()
tensor_train = torch_tensor(train, dtype=torch_float())
tensor_target = torch_tensor(target, dtype=torch_float())
model_SF = nn_sequential(
#Layer 1
nn_linear(3,2),  #we have 3 features and we will add 2 neurons to our hidden layer
nn_relu(),       #the reLU or rectified linear unit function
#Layer 2
nn_linear(2,1)   #takes the output of layer 1 and linearizes it ready for the sigmoid in the nn_bce_with_logits_loss
#Layer 3 - is built-in our criteria this time
)
pred_temp = model_SF(tensor_train)
# Define cost and optimizer
criterion = nn_bce_with_logits_loss()
optimizer = optim_adam(model_SF$parameters, lr = 0.01)
epochs = 2000
# Train the net
for(i in 1:epochs){
optimizer$zero_grad()
y_pred = model_SF(tensor_train)
y_pred_class = (y_pred>0.5)
loss = criterion(y_pred, tensor_target)
loss$backward()
optimizer$step()
# Check Training
if(i %% 100 == 0){
corrects = (y_pred_class == tensor_target)
accuracy = corrects$sum()$item() / tensor_target$size(1)
cat(" Epoch:", i,"Loss: ", loss$item()," Accuracy:",accuracy,"\n")
}
}
y_pred = as.numeric(model_SF(tensor_train)>0.5)
quality_measures(y_pred, target)
test = as.matrix(testing[,c("price_scale","elevation_scale","sqft_scale")])
tensor_test = torch_tensor(test, dtype=torch_float())
y_test = as.numeric(model_SF(tensor_test)>0.5)
quality_measures(y_test, testing$in_sf)
library(randomForest)
set.seed(2020)
rf = randomForest(factor(in_sf) ~ price_scale + elevation_scale + sqft_scale, data = training)
rf
predRF = predict(rf, newdata = testing)
quality_measures(predRF, testing$in_sf)
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
library(torch)
circuit8 = as.data.frame(expand.grid(0:1, 0:1, 0:1, 0:1, 0:1, 0:1, 0:1, 0:1))
TF = (circuit8 == 1)
circ_out = as.numeric(((TF[,1] & TF[,2]) | ((TF[,3] | TF[,4]))) & ((TF[,5] & TF[,6]) & (TF[,7] | TF[,8])))
circuit8$out = circ_out
head(circuit8)
torch_train = torch_tensor(    #use torch_tensor to convert your data
as.matrix(circuit8[,1:8]), #take the first 8 columns containing the features we will use for training
#and convert the data frame to a matrix using as.matrix
dtype = torch_float())     #choose the type of data in your matrix, it is almost always a float
torch_target = torch_tensor(
as.numeric(circuit8[,9]),  #take the last column with the target response values and ensure they are numeric
dtype = torch_float())
criterion = nn_bce_loss() #binary cross-entropy (bce)
set.seed(42)
model = nn_sequential(
nn_linear(8,1),
nn_sigmoid()
)
optimizer = optim_adam(model$parameters, lr = 0.01) #gradient descent on steroids with a learning rate of 0.01
epochs = 2000 #many epochs
for (i in 1:epochs) {
optimizer$zero_grad()  #we don't want the gradients to be accummulated but reset in each epoch, there'll be cases where this will be needed
y_pred = model(torch_train) #fit the current model, return predicted values
loss = criterion(y_pred, torch_target) #calculate loss
loss$backward() #backward propagate your losses
optimizer$step()    #next step
if (i %% 100 == 0) {  #some verbose to keep track of how well your neural net is being trained
corrects <- (as.numeric(y_pred > 0.5) == torch_target)
accuracy <- corrects$sum()$item()/torch_target$size()
cat("Epoch:", i, "Loss:", loss$item(), "Accuracy:", accuracy, "\n")
}
}
quality_measures = function(pred, actual){
confusion = table(pred, actual)
accuracy = sum(diag(confusion))/sum(confusion) #accuracy - proportion of correctly classified
prec = confusion[2,2]/sum(confusion[,2]) # precision - positive predictive value
sens = confusion[2,2]/sum(confusion[2,]) # sensitivity - true positive rate
spec = confusion[1,1]/sum(confusion[1,]) # specificity - true negative rate
fscore = 2*(sens*prec)/(sens + prec) #F-score - measure of predictive performance
print(c("Precision: ", prec))
print(c("Sensitivity: ", sens))
print(c("Specificity: ", spec))
print(c("F Score: ", fscore))
print(c("Accuracy: ", accuracy))
print(confusion)
}
pred_values = as.numeric(model(torch_train)>0.5)
quality_measures(pred_values, as.numeric(torch_target))
set.seed(42)
model_3layer = nn_sequential(
#Input layer
nn_linear(8,4), #we choose to have 4 neurons in our hidden layer so the input layer needs to return 4 outputs
#Hidden layer
nn_linear(4,1), #4 neurons combined linearly to output one value
#Output layer
nn_sigmoid()
)
optimizer = optim_adam(model_3layer$parameters, lr = 0.01) #gradient descent on steroids with a learning rate of 0.01
epochs = 1000 #many epochs
for (i in 1:epochs) {
optimizer$zero_grad()  #we don't want the gradients to be accummulated but reset in each epoch, there'll be cases where this will be needed
y_pred = model_3layer(torch_train) #fit the current model, return predicted values
loss = criterion(y_pred, torch_target) #calculate loss
loss$backward() #backward propagate your losses
optimizer$step()    #next step
if (i %% 100 == 0) {  #some verbose to keep track of how well your neural net is being trained
corrects = (as.numeric(y_pred > 0.5) == torch_target)
accuracy = corrects$sum()$item()/torch_target$size()
cat("Epoch:", i, "Loss:", loss$item(), "Accuracy:", accuracy, "\n")
}
}
pred_values = as.numeric(model_3layer(torch_train)>0.5)
quality_measures(pred_values, as.numeric(torch_target))
model_3layer$parameters
library(dplyr)
file = "part_1_data.csv"
houses = read.csv(file,header=TRUE)
head(houses)
library(scales)
houses$price_scale = rescale(houses$price)
houses$elevation_scale = rescale(houses$elevation)
houses$sqft_scale = rescale(houses$sqft)
head(houses)
set.seed(2024)
ind = sample(1:nrow(houses), 350) #about 75% for training
training = houses[ind,]
testing = houses[-ind,]
train = as.matrix(training[,c("price_scale","elevation_scale","sqft_scale")])
target = as.matrix(training$in_sf)
criterion = nn_bce_with_logits_loss()
tensor_train = torch_tensor(train, dtype=torch_float())
tensor_target = torch_tensor(target, dtype=torch_float())
model_SF = nn_sequential(
#Layer 1
nn_linear(3,2),  #we have 3 features and we will add 2 neurons to our hidden layer
nn_relu(),       #the reLU or rectified linear unit function
#Layer 2
nn_linear(2,1)   #takes the output of layer 1 and linearizes it ready for the sigmoid in the nn_bce_with_logits_loss
#Layer 3 - is built-in our criteria this time
)
pred_temp = model_SF(tensor_train)
# Define cost and optimizer
criterion = nn_bce_with_logits_loss()
optimizer = optim_adam(model_SF$parameters, lr = 0.01)
epochs = 2000
# Train the net
for(i in 1:epochs){
optimizer$zero_grad()
y_pred = model_SF(tensor_train)
y_pred_class = (y_pred>0.5)
loss = criterion(y_pred, tensor_target)
loss$backward()
optimizer$step()
# Check Training
if(i %% 100 == 0){
corrects = (y_pred_class == tensor_target)
accuracy = corrects$sum()$item() / tensor_target$size(1)
cat(" Epoch:", i,"Loss: ", loss$item()," Accuracy:",accuracy,"\n")
}
}
y_pred = as.numeric(model_SF(tensor_train)>0.5)
quality_measures(y_pred, target)
test = as.matrix(testing[,c("price_scale","elevation_scale","sqft_scale")])
tensor_test = torch_tensor(test, dtype=torch_float())
y_test = as.numeric(model_SF(tensor_test)>0.5)
quality_measures(y_test, testing$in_sf)
library(randomForest)
set.seed(2020)
rf = randomForest(factor(in_sf) ~ price_scale + elevation_scale + sqft_scale, data = training)
rf
predRF = predict(rf, newdata = testing)
quality_measures(predRF, testing$in_sf)
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
library(dplyr)
# current directory
current_directory = getwd()
# read_csv
# joint file path
file = file.path(current_directory, "./abalone/abalone.data")
abalone_origin = read.csv(file, header = TRUE)
head(abalone_origin)
# --------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# --------------------------------------------------------
library(dplyr)
# current directory
current_directory = getwd()
# read_csv
# joint file path
file = file.path(current_directory, "./abalone/abalone.data")
abalone_origin = read.csv(file, header = FALSE)
head(abalone_origin)
View(abalone_origin)
# --------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# --------------------------------------------------------
library(dplyr)
# current directory
current_directory = getwd()
# read_csv
# joint file path
file = file.path(current_directory, "./abalone/abalone.data")
abalone_origin = read.csv(file, header = TRUE)
head(abalone_origin)
# --------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# --------------------------------------------------------
library(dplyr)
# current directory
current_directory = getwd()
# read_csv
# joint file path
file = file.path(current_directory, "./abalone/abalone.data")
abalone_origin = read.csv(file, header = FALSE)
head(abalone_origin)
library(dplyr)
# current directory
current_directory = getwd()
# read_csv
# joint file path
file = file.path(current_directory, "./abalone/abalone.data")
abalone_origin = read.csv(file, header = FALSE)
# Joint file path for .names file
names_file <- file.path(current_directory, "abalone/abalone.names")
# Read the .names file to get column names
# Assuming the .names file contains one column name per line
column_names <- readLines(names_file)
# Assign column names to the dataframe
colnames(abalone_origin) <- column_names
# --------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# --------------------------------------------------------
# --------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# --------------------------------------------------------
library(dplyr)
# current directory
current_directory = getwd()
# read_csv
# joint file path
file = file.path(current_directory, "./abalone/abalone.data")
abalone_origin = read.csv(file, header = FALSE)
# Manually set the column names
colnames(abalone_origin) <- c("Sex", "Length", "Diameter", "Height", "WholeWeight", "ShuckedWeight", "VisceraWeight", "ShellWeight", "Rings")
# Display the first few rows of the dataframe to verify
head(abalone_origin)
View(abalone_origin)
# install.packages('dplyr')
# --------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# --------------------------------------------------------
library(dplyr)
# current directory
current_directory = getwd()
# read_csv
# joint file path
file = file.path(current_directory, "./abalone/abalone.data")
abalone_origin = read.csv(file, header = FALSE)
# Manually set the column names
colnames(abalone_origin) <- c("Sex", "Length", "Diameter", "Height", "WholeWeight", "ShuckedWeight", "VisceraWeight", "ShellWeight", "Rings")
# Add a new column 'Age' which is 'Rings' + 1.5
abalone_origin <- abalone_origin %>%
mutate(Age = Rings + 1.5)
# Display the first few rows of the dataframe to verify
head(abalone_origin)
View(abalone_origin)

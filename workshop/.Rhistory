graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
# current_directory
current_directory = getwd()
# read_csv
# joint file path
file_path = file.path(current_directory, "auto-mpg.csv")
autompg = read.csv(file_path, header=TRUE)
library(dplyr)
autompgclean = autompg %>% mutate(horsepower = as.numeric(horsepower)) %>% na.omit %>% select(-name)
library("ggplot2")
library("GGally")
ggpairs(autompgclean)+theme_bw()
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
# current_directory
current_directory = getwd()
# read_csv
# joint file path
file_path = file.path(current_directory, "auto-mpg.csv")
autompgCSV = read.csv(file_path, header=TRUE)
head(autompgCSV)
sort(unique(autompgCSV$horsepower))
as.numeric(sort(unique(autompgCSV$horsepower)))
autompgCSV$horsepower = as.numeric(autompgCSV$horsepower)
head(autompgCSV)
library(dplyr) # https://dplyr.tidyverse.org/index.html
autompg = autompgCSV %>% mutate(horsepower = as.numeric(horsepower))
head(autompg)
# Method 1 - using complete.cases and pipes
autompg %>% filter(!complete.cases(.))
# Method 2 - using complete.cases and base R
# autompg[!complete.cases(autompg),]
#Method 1 - using complete.cases and pipes
#autompg %>% filter(complete.cases(.))
#Method 2 - using complete.cases and base R
#autompg[complete.cases(autompg),]
#Method 3 - using na.omit
#na.omit(autompg)
#Method 4 - using na.omit and pipes
autompg %>% na.omit
autompgclean = autompg %>% na.omit %>% select(-name)
head(autompgclean)
library("ggplot2")
library("GGally")
ggpairs(autompgclean)+theme_bw()
# theme_bw is one of many themes you can modify from ggplot
# see https://ggplot2.tidyverse.org/reference/ggtheme.html for other themes
ols_model = lm(mpg ~ horsepower + displacement + weight + acceleration, data=autompgclean)
summary(ols_model)
library(caret)
library(glmnet)
lambdas=10^seq(-3,3,by=0.1)
ctrl_kfold = trainControl(method = "cv", number = 10)
ridge_kfold = train(mpg ~ horsepower + displacement + weight + acceleration,
data = autompgclean,
method = "glmnet",
metric = "RMSE",
tuneGrid = expand.grid(alpha = 0,
lambda = lambdas),
trControl = ctrl_kfold,
thresh=1e-10)
print(ridge_kfold$finalModel$lambdaOpt)
coef(ridge_kfold$finalModel, ridge_kfold$bestTune$lambda)
head(ridge_kfold$results)
cv_ridgeplot = ggplot(ridge_kfold$results) + #choose the dataset that contains the variables we want to use
geom_point(aes(x = lambda, y = RMSE), color = "red") + #plot the RMSE for each lambda in red
scale_x_log10(labels = function(x) format(x, scientific = FALSE)) + #change the x-axis label to log scale and remove scientific notation
geom_errorbar(aes(x = lambda, ymin = RMSE - RMSESD, ymax = RMSE + RMSESD), width = 0.3, color = "lightgray") + #add errorbars using the CV standard deviation for the RMSE
theme_bw() #select the bw theme
cv_ridgeplot
Ridge_lambda = ridge_kfold$bestTune$lambda
RMSE_Ridge = ridge_kfold$results %>% filter(lambda == Ridge_lambda)
RMSE_Ridge
cv_ridgeplot +
geom_point(data=RMSE_Ridge, aes(x=lambda, y=RMSE), colour = "purple4", size = 3) +
geom_label(
data= RMSE_Ridge,
label="Winner",
aes(x=lambda, y=RMSE),
nudge_x = 0.25,
nudge_y = 0.25
)
lambdas=seq(0.01,2,by=0.01)
ctrl_kfold = trainControl(method = "cv", number = 10)
lasso_kfold = train(mpg ~ horsepower + displacement + weight + acceleration,
data = autompgclean,
method = "glmnet",
metric = "RMSE",
tuneGrid = expand.grid(alpha = 1,
lambda = lambdas),
trControl = ctrl_kfold,
thresh=1e-10)
print(lasso_kfold$finalModel$lambdaOpt)
coef(lasso_kfold$finalModel, lasso_kfold$bestTune$lambda)
cv_lassoplot = ggplot(lasso_kfold$results) + #choose the dataset that contains the variables we want to use
geom_point(aes(x = lambda, y = RMSE), color = "red") + #plot the RMSE for each lambda in red
#scale_x_log10(labels = function(x) format(x, scientific = FALSE)) + #change the x-axis label to log scale and remove scientific notation
geom_errorbar(aes(x = lambda, ymin = RMSE - RMSESD, ymax = RMSE + RMSESD), width = 0.3, color = "lightgray") + #add errorbars using the CV standard deviation for the RMSE
theme_bw() #select the bw theme
cv_lassoplot
lasso_lambda = lasso_kfold$bestTune$lambda
RMSE_lasso = lasso_kfold$results %>% filter(lambda == lasso_lambda)
RMSE_lasso
cv_lassoplot +
geom_point(data=RMSE_lasso, aes(x=lambda, y=RMSE), colour = "purple4", size = 3) +
geom_label(
data= RMSE_lasso,
label="Winner?",
aes(x=lambda, y=RMSE),
nudge_x = 0.1,
nudge_y = 0.1
)
# ---------------------------------------------------------------------
# Best Subset Selection methods
library(leaps)
best_subset_selection = regsubsets(mpg ~ ., data=autompgclean)
summary(best_subset_selection)
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
# current_directory
current_directory = getwd()
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
# current_directory
current_directory = getwd()
# read_csv
# joint file path
file_path = file.path(current_directory, "auto-mpg.csv")
autompgCSV = read.csv(file_path, header=TRUE)
head(autompgCSV)
sort(unique(autompgCSV$horsepower))
as.numeric(sort(unique(autompgCSV$horsepower)))
autompgCSV$horsepower = as.numeric(autompgCSV$horsepower)
head(autompgCSV)
library(dplyr) # https://dplyr.tidyverse.org/index.html
autompg = autompgCSV %>% mutate(horsepower = as.numeric(horsepower))
head(autompg)
# Method 1 - using complete.cases and pipes
autompg %>% filter(!complete.cases(.))
# Method 2 - using complete.cases and base R
# autompg[!complete.cases(autompg),]
#Method 1 - using complete.cases and pipes
#autompg %>% filter(complete.cases(.))
#Method 2 - using complete.cases and base R
#autompg[complete.cases(autompg),]
#Method 3 - using na.omit
#na.omit(autompg)
#Method 4 - using na.omit and pipes
autompg %>% na.omit
autompgclean = autompg %>% na.omit %>% select(-name)
head(autompgclean)
library("ggplot2")
library("GGally")
ggpairs(autompgclean)+theme_bw()
# theme_bw is one of many themes you can modify from ggplot
# see https://ggplot2.tidyverse.org/reference/ggtheme.html for other themes
ols_model = lm(mpg ~ horsepower + displacement + weight + acceleration, data=autompgclean)
summary(ols_model)
library(caret)
library(glmnet)
lambdas=10^seq(-3,3,by=0.1)
ctrl_kfold = trainControl(method = "cv", number = 10)
ridge_kfold = train(mpg ~ horsepower + displacement + weight + acceleration,
data = autompgclean,
method = "glmnet",
metric = "RMSE",
tuneGrid = expand.grid(alpha = 0,
lambda = lambdas),
trControl = ctrl_kfold,
thresh=1e-10)
print(ridge_kfold$finalModel$lambdaOpt)
coef(ridge_kfold$finalModel, ridge_kfold$bestTune$lambda)
head(ridge_kfold$results)
cv_ridgeplot = ggplot(ridge_kfold$results) + #choose the dataset that contains the variables we want to use
geom_point(aes(x = lambda, y = RMSE), color = "red") + #plot the RMSE for each lambda in red
scale_x_log10(labels = function(x) format(x, scientific = FALSE)) + #change the x-axis label to log scale and remove scientific notation
geom_errorbar(aes(x = lambda, ymin = RMSE - RMSESD, ymax = RMSE + RMSESD), width = 0.3, color = "lightgray") + #add errorbars using the CV standard deviation for the RMSE
theme_bw() #select the bw theme
cv_ridgeplot
Ridge_lambda = ridge_kfold$bestTune$lambda
RMSE_Ridge = ridge_kfold$results %>% filter(lambda == Ridge_lambda)
RMSE_Ridge
cv_ridgeplot +
geom_point(data=RMSE_Ridge, aes(x=lambda, y=RMSE), colour = "purple4", size = 3) +
geom_label(
data= RMSE_Ridge,
label="Winner",
aes(x=lambda, y=RMSE),
nudge_x = 0.25,
nudge_y = 0.25
)
lambdas=seq(0.01,2,by=0.01)
ctrl_kfold = trainControl(method = "cv", number = 10)
lasso_kfold = train(mpg ~ horsepower + displacement + weight + acceleration,
data = autompgclean,
method = "glmnet",
metric = "RMSE",
tuneGrid = expand.grid(alpha = 1,
lambda = lambdas),
trControl = ctrl_kfold,
thresh=1e-10)
print(lasso_kfold$finalModel$lambdaOpt)
coef(lasso_kfold$finalModel, lasso_kfold$bestTune$lambda)
cv_lassoplot = ggplot(lasso_kfold$results) + #choose the dataset that contains the variables we want to use
geom_point(aes(x = lambda, y = RMSE), color = "red") + #plot the RMSE for each lambda in red
#scale_x_log10(labels = function(x) format(x, scientific = FALSE)) + #change the x-axis label to log scale and remove scientific notation
geom_errorbar(aes(x = lambda, ymin = RMSE - RMSESD, ymax = RMSE + RMSESD), width = 0.3, color = "lightgray") + #add errorbars using the CV standard deviation for the RMSE
theme_bw() #select the bw theme
cv_lassoplot
lasso_lambda = lasso_kfold$bestTune$lambda
RMSE_lasso = lasso_kfold$results %>% filter(lambda == lasso_lambda)
RMSE_lasso
cv_lassoplot +
geom_point(data=RMSE_lasso, aes(x=lambda, y=RMSE), colour = "purple4", size = 3) +
geom_label(
data= RMSE_lasso,
label="Winner?",
aes(x=lambda, y=RMSE),
nudge_x = 0.1,
nudge_y = 0.1
)
source("~/.active-rstudio-document", echo=TRUE)
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
# current directory
current_directory = getwd()
# read_csv
# joint file path
file = file.path(current_directory, "part_1_data.csv")
library(dplyr)
# file = "part_1_data.csv"
houses = read.csv(file,header=TRUE) %>%
mutate(city = as.factor(case_when(
in_sf == 1 ~ "SF",
in_sf == 0 ~ "NY"))) %>%
select(-in_sf)
head(houses)
summary(lm(price ~ sqft, data = houses))
library(caret)
n_sample = 1000 #choose the number of bootstrap samples you want to take
b_sample_size = 440 #set the sample size for each bootstrap sample
RMSE_boot = rep(NA, n_sample) #pre-allocate a vector to save the calculated RMSE values
for (b in 1:n_sample) {
ind = sample(1:nrow(houses), size = b_sample_size, replace = TRUE) #take a sample of size b_sample_size with replacement
train = houses[ind, ] #create your training dataset - this subset will always have b_sample_size observations
test = houses[-ind, ] #create your out-of-bag or testing dataset - the size of this subset will vary depending on the number of repetitions in train
mdl = lm(price ~ sqft, data = train) #train your model
pred = predict(mdl, newdata = test) #try to predict using the out-of-bag samples
RMSE_boot[b] = RMSE(pred, test$price) #calculate the relevant estimate you want to investigate
}
summary(RMSE_boot)
hist(RMSE_boot)
n_sample = 2000 #choose the number of bootstrap samples you want to take
b_sample_size = 440 #set the sample size for each bootstrap sample
RMSE_boot = rep(NA, n_sample) #pre-allocate a vector to save the calculated RMSE values
for (b in 1:n_sample) {
ind = sample(1:nrow(houses), size = b_sample_size, replace = TRUE) #take a sample of size b_sample_size with replacement
train = houses[ind, ] #create your training dataset - this subset will always have b_sample_size observations
test = houses[-ind, ] #create your out-of-bag or testing dataset - the size of this subset will vary depending on the number of repetitions in train
mdl = lm(price ~ sqft, data = train) #train your model
pred = predict(mdl, newdata = test) #try to predict using the out-of-bag samples
RMSE_boot[b] = RMSE(pred, test$price) #calculate the relevant estimate you want to investigate
}
summary(RMSE_boot)
hist(RMSE_boot)
library(rpart)
set.seed(42)
ind = sample(1:nrow(houses), size = 442)
train_houses = houses[ind, ] # 442 samples for training
test_houses = houses[-ind, ] # 50 samples for validation
regpart = rpart(data = train_houses, price ~ sqft, xval = 0) #xval turns cross-validation off
pred_single = predict(regpart, newdata = test_houses) #use your regression tree to predict prices on the testing dataset
calc_RMSE = RMSE(pred_single, test_houses$price) #calculate the RMSE
noquote(c("The calculated RMSE is:", calc_RMSE)) #noquote is like print without the quotes
pred_test = test_houses %>% mutate(pred_prices = pred_single)
library(ggplot2)
ggplot(pred_test)
+ geom_point(aes(x = price, y = pred_prices))
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
# current directory
current_directory = getwd()
# read_csv
# joint file path
file = file.path(current_directory, "part_1_data.csv")
library(dplyr)
# file = "part_1_data.csv"
houses = read.csv(file,header=TRUE) %>%
mutate(city = as.factor(case_when(
in_sf == 1 ~ "SF",
in_sf == 0 ~ "NY"))) %>%
select(-in_sf)
head(houses)
summary(lm(price ~ sqft, data = houses))
library(caret)
n_sample = 1000 #choose the number of bootstrap samples you want to take
b_sample_size = 440 #set the sample size for each bootstrap sample
RMSE_boot = rep(NA, n_sample) #pre-allocate a vector to save the calculated RMSE values
for (b in 1:n_sample) {
ind = sample(1:nrow(houses), size = b_sample_size, replace = TRUE) #take a sample of size b_sample_size with replacement
train = houses[ind, ] #create your training dataset - this subset will always have b_sample_size observations
test = houses[-ind, ] #create your out-of-bag or testing dataset - the size of this subset will vary depending on the number of repetitions in train
mdl = lm(price ~ sqft, data = train) #train your model
pred = predict(mdl, newdata = test) #try to predict using the out-of-bag samples
RMSE_boot[b] = RMSE(pred, test$price) #calculate the relevant estimate you want to investigate
}
summary(RMSE_boot)
hist(RMSE_boot)
n_sample = 2000 #choose the number of bootstrap samples you want to take
b_sample_size = 440 #set the sample size for each bootstrap sample
RMSE_boot = rep(NA, n_sample) #pre-allocate a vector to save the calculated RMSE values
for (b in 1:n_sample) {
ind = sample(1:nrow(houses), size = b_sample_size, replace = TRUE) #take a sample of size b_sample_size with replacement
train = houses[ind, ] #create your training dataset - this subset will always have b_sample_size observations
test = houses[-ind, ] #create your out-of-bag or testing dataset - the size of this subset will vary depending on the number of repetitions in train
mdl = lm(price ~ sqft, data = train) #train your model
pred = predict(mdl, newdata = test) #try to predict using the out-of-bag samples
RMSE_boot[b] = RMSE(pred, test$price) #calculate the relevant estimate you want to investigate
}
summary(RMSE_boot)
hist(RMSE_boot)
library(rpart)
set.seed(42)
ind = sample(1:nrow(houses), size = 442)
train_houses = houses[ind, ] # 442 samples for training
test_houses = houses[-ind, ] # 50 samples for validation
regpart = rpart(data = train_houses, price ~ sqft, xval = 0) #xval turns cross-validation off
pred_single = predict(regpart, newdata = test_houses) #use your regression tree to predict prices on the testing dataset
calc_RMSE = RMSE(pred_single, test_houses$price) #calculate the RMSE
noquote(c("The calculated RMSE is:", calc_RMSE)) #noquote is like print without the quotes
pred_test = test_houses %>% mutate(pred_prices = pred_single)
library(ggplot2)
ggplot(pred_test)
+ geom_point(aes(x = price, y = pred_prices))
pred_test = test_houses %>% mutate(pred_prices = pred_single)
library(ggplot2)
ggplot(pred_test) +
geom_point(aes(x = price, y = pred_prices)) +
geom_abline(intercept = 0,
slope = 1,
colour = "red")
pred_test = test_houses %>% mutate(pred_prices = pred_single)
library(ggplot2)
ggplot(pred_test) +
geom_point(aes(x = price, y = pred_prices)) +
geom_abline(intercept = 0,
slope = 1,
colour = "red")
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
# current directory
current_directory = getwd()
# read_csv
# joint file path
file = file.path(current_directory, "part_1_data.csv")
library(dplyr)
# file = "part_1_data.csv"
houses = read.csv(file,header=TRUE) %>%
mutate(city = as.factor(case_when(
in_sf == 1 ~ "SF",
in_sf == 0 ~ "NY"))) %>%
select(-in_sf)
head(houses)
summary(lm(price ~ sqft, data = houses))
library(caret)
n_sample = 1000 #choose the number of bootstrap samples you want to take
b_sample_size = 440 #set the sample size for each bootstrap sample
RMSE_boot = rep(NA, n_sample) #pre-allocate a vector to save the calculated RMSE values
for (b in 1:n_sample) {
ind = sample(1:nrow(houses), size = b_sample_size, replace = TRUE) #take a sample of size b_sample_size with replacement
train = houses[ind, ] #create your training dataset - this subset will always have b_sample_size observations
test = houses[-ind, ] #create your out-of-bag or testing dataset - the size of this subset will vary depending on the number of repetitions in train
mdl = lm(price ~ sqft, data = train) #train your model
pred = predict(mdl, newdata = test) #try to predict using the out-of-bag samples
RMSE_boot[b] = RMSE(pred, test$price) #calculate the relevant estimate you want to investigate
}
summary(RMSE_boot)
hist(RMSE_boot)
n_sample = 2000 #choose the number of bootstrap samples you want to take
b_sample_size = 440 #set the sample size for each bootstrap sample
RMSE_boot = rep(NA, n_sample) #pre-allocate a vector to save the calculated RMSE values
for (b in 1:n_sample) {
ind = sample(1:nrow(houses), size = b_sample_size, replace = TRUE) #take a sample of size b_sample_size with replacement
train = houses[ind, ] #create your training dataset - this subset will always have b_sample_size observations
test = houses[-ind, ] #create your out-of-bag or testing dataset - the size of this subset will vary depending on the number of repetitions in train
mdl = lm(price ~ sqft, data = train) #train your model
pred = predict(mdl, newdata = test) #try to predict using the out-of-bag samples
RMSE_boot[b] = RMSE(pred, test$price) #calculate the relevant estimate you want to investigate
}
summary(RMSE_boot)
hist(RMSE_boot)
library(rpart)
set.seed(42)
ind = sample(1:nrow(houses), size = 442)
train_houses = houses[ind, ] # 442 samples for training
test_houses = houses[-ind, ] # 50 samples for validation
regpart = rpart(data = train_houses, price ~ sqft, xval = 0) #xval turns cross-validation off
pred_single = predict(regpart, newdata = test_houses) #use your regression tree to predict prices on the testing dataset
calc_RMSE = RMSE(pred_single, test_houses$price) #calculate the RMSE
noquote(c("The calculated RMSE is:", calc_RMSE)) #noquote is like print without the quotes
pred_test = test_houses %>% mutate(pred_prices = pred_single)
library(ggplot2)
ggplot(pred_test) +
geom_point(aes(x = price, y = pred_prices)) +
geom_abline(intercept = 0,
slope = 1,
colour = "red")
n_sample = 500
b_sample_size = 400
greta = list()
RMSE_boot = rep(NA,n_sample)
for (b in 1:n_sample){
ind = sample(1:nrow(train_houses),size = b_sample_size, replace = TRUE)
train = train_houses[ind,] #training sample, size 400
test = train_houses[-ind,] #out-of-bag
regpart = rpart(data = train, price ~ sqft, xval = 0)
greta[[b]] = regpart #save the trees in a list
pred = predict(regpart, newdata = test)
RMSE_boot[b] = RMSE(pred, test$price)
}
print(mean(RMSE_boot))
price_boot = matrix(NA, nrow = nrow(test_houses), ncol = n_sample)
RMSE_pred_boot = rep(NA, n_sample)
for (b in 1:n_sample){
pred = predict(greta[[b]],newdata = test_houses)
price_boot[,b] = pred
RMSE_pred_boot[b] = RMSE(pred, test_houses$price)
}
mean_pred_prices = rowMeans(price_boot)
mean(RMSE_pred_boot)
pred_test_bag = pred_test %>% mutate(pred_bag = mean_pred_prices)
ggplot(pred_test_bag) + geom_point(aes(x=price, y=pred_bag)) + geom_abline(intercept = 0,slope = 1, colour = "red")
ggplot(pred_test_bag) +
geom_point(aes(x=price, y=pred_bag)) + # bagged model
geom_abline(intercept = 0,slope = 1, colour = "red") +
geom_point(aes(x=price, y=pred_prices), colour = "blue") # First model
# Matplot is easier and faster than ggplot for this task... not necessarily prettier
matplot(price_boot, pred_test_bag$price, col="grey", pch = 16)
points(pred_test_bag$pred_bag, pred_test_bag$price, col="black", pch = 16)
points(pred_test_bag$pred_prices, pred_test_bag$price, col = "blue", pch = 16)
abline(0,1, col="red")
fitControl = trainControl(method = "cv", number = 10)
BagTrees = train(price ~ sqft,
data = train_houses,
method = "treebag",
nbagg = 500,
trControl = fitControl)
BagTrees #Take a break, this bit can take a minute or two. You are cross-validating and bootstrapping here!
predCaretBags = test_houses %>% mutate(predBagCV = predict(BagTrees, newdata = test_houses))
ggplot(predCaretBags) + geom_point(aes(x=price, y=predBagCV)) + geom_abline(intercept = 0,slope = 1, colour = "red")
library(randomForest)
set.seed(42)
ind = sample(1:nrow(houses), size = 50)
test_houses = houses[ind,]
train_houses = houses[-ind, ]
rf = randomForest(price ~ sqft, data = train_houses)
rf
predRF = test_houses %>% mutate(pred_rf = predict(rf, newdata = test_houses))
RMSE(predRF$pred_rf, predRF$price)
ggplot(predRF) + geom_point(aes(x=price, y=pred_rf)) + geom_abline(intercept = 0,slope = 1, colour = "red")
rf = randomForest(price ~ ., data = train_houses, mtry = 5)
rf
predRF = test_houses %>% mutate(pred_rf = predict(rf, newdata = test_houses))
RMSE(predRF$pred_rf, predRF$price)
ggplot(predRF) + geom_point(aes(x=price, y=pred_rf)) + geom_abline(intercept = 0,slope = 1, colour = "red")
rf = randomForest(factor(city) ~ sqft, data = train_houses)
rf
pred = predict(rf, newdata = test_houses)
confusionMatrix(pred, factor(test_houses$city))
rf = randomForest(factor(city) ~ ., data = train_houses)
rf
pred = predict(rf, newdata = test_houses)
confusionMatrix(pred, factor(test_houses$city))

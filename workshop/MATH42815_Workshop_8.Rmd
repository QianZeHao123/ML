#### MATH42815 - Machine Learning - 2023/24

## Machine Learning - Workshop 8

**Aim**: The aim of this workshop is to introduce Random Forests.

*''Pippin: It's talking, Merry. The tree is talking.*

*Treebeard: Tree?! I am no tree! I am an Ent.''* - Tolkien, The Lord of the Rings: The Two Towers

### Introduction

Random Forest is an ensemble method. An ensemble is a combination of predictions/outputs from multiple models into a single output; this approach often leads to higher accuracy and improved robustness.

The most commonly used processes for combining these outputs are: stacking, boosting, and bagging.

-   **Stacking:** the predictions or outputs of the individual (learner) models act as input for another model, called a meta-model. The meta-model can be a linear regression, a neural network, etc.

-   **Boosting:** models are trained successively; after each model (or set of models) is trained, the next model tries to correct the errors of its predecessor. If the problem being addressed is a classification problem, the misclassified points from a model will be passed to the next model with bigger weights to the next model. In the case of a regression problem, a new model is fitted to the residuals of the previous model. AdaBoost or Adaptive Boosting is the most commonly used type of boosting for classification while Gradient Boosting is the most common boosting algorithm used for regression problems.

-   **Bagging:** Unrelated to Bilbo Baggins, Bagging is short for Bootstrap Aggregating. This is the most commonly used method for creating ensembles. Multiple samples from the training data are created and a model is created in each of those samples. The sampling procedured used is called the Bootstrap where samples are drawn randomly with replacement. The ensemble is normally aggregated by voting in case of classification problems or by averaging of predictions in the case of regression problems. Bagging is particularly useful for reducing overfitting and stabilizing high variance models such as decision trees.

In this workshop we will introduce you to **Bagging**, **Bootstrap**, and **Random Forests**.

### Boostrapping and Bagging

*''We must all keep together, and not risk getting separated. All of us must escape, or none and this is our last chance.''* - Bilbo Baggins, The Hobbit by Tolkien

The **boostrap** is a resampling technique published in 1979 by Efron as an extension of another resampling algorithm called the **jackknife**.

You have already used jackknife in this course when you wrote your own algorithm for leave-one-out cross-validation; the jackknife is the process of drawing $n$ samples of size $(n-1)$ from a dataset with $n$ observations, calculating a relevant statistic using each subsample and then averaging them. In the LOOCV, you fit $n$ models each using $n-1$ samples from the training dataset then calculate the MSE based on the prediction for the remaining sample, then take the average of the calculated MSEs.

The samples used for the jackknife (and the LOOCV) are not drawn at random. For a dataset with $n$ observations, $n$ subsets are created. For each subset $i$, the $i$-th observation is removed from the initial dataset creating $n$ subsets with $(n-1)$ observations each.

For the **bootstrap**, we draw randomly $n_{samples}$ samples of size $n_{boot}$ with replacement. We treat those samples as if they were real samples from the population your data came from originally. What you do with your samples next depends on what information you need to extract/retain from modelling each sample.

One of the things you can do with your bootstrap samples is to aggregate the output of models you have fitted to them.

#### Bagging

Start with a dataset with $n$ observations. Choose $n_{samples}$, the number of boostrap samples you plan to draw; $n_{samples}=1000$ is often a good start. Choose the size $n_{boot}$ of the samples you will draw. Ideally $n_{boot}\leq n$.

For $b = 1, 2, 3, \ldots, n_{samples}$:

-   Draw a random sample $S_b$ of size $n_{boot}$ with replacement

-   Identify all observations from the original dataset that are not in $S$, save those observations. They are called the **out-of-bag** sample.

-   Train a model using the bootstrapped sample $S_b$.

-   Use the **out-of-bag** samples as the testing dataset for the model you have trained.

-   Return the relevant measure you want to use to assess model quality (e.g. accuracy, RMSE)

Once you have repeated the steps above for all $n_{samples}$, average the measure you have calculated for each sample and return the bagged result.

Now let's get some of these concepts applied in practice. We will use the same dataset we used in Workshop 7. The dataset has 8 variables and it contains information on 492 properties in New York and San Francisco.

```{r}
# ---------------------------------------------------------------------
# clear the environment var area
rm(list = ls())
# clear all plots
graphics.off()
# clear the console area
cat("\014")
# ---------------------------------------------------------------------
# current directory
current_directory = getwd()
# read_csv
# joint file path
file = file.path(current_directory, "part_1_data.csv")
```

```{r}
library(dplyr)
# file = "part_1_data.csv"
houses = read.csv(file,header=TRUE) %>%
                            mutate(city = as.factor(case_when(
                                                    in_sf == 1 ~ "SF",
                                                    in_sf == 0 ~ "NY"))) %>%
                            select(-in_sf)
head(houses)
```

We can fit a simple linear regression to model *price* as a function of *sqft*.

```{r}
summary(lm(price ~ sqft, data = houses))
```

We can estimate the RMSE for this model using bootstrap. The code below follows the steps in the pseudo-code above. Instead of returning just the mean of the calculated RMSEs, we can look at other summaries and the histogram of the RMSEs.

**Exercise:** Try changing the *n_sample* and *b_sample_size* and investigate the impact of those two values in the estimates you have produced:

```{r}
library(caret)
```

```{r}
n_sample = 1000 #choose the number of bootstrap samples you want to take
b_sample_size = 440 #set the sample size for each bootstrap sample
RMSE_boot = rep(NA, n_sample) #pre-allocate a vector to save the calculated RMSE values
for (b in 1:n_sample) {
  ind = sample(1:nrow(houses), size = b_sample_size, replace = TRUE) #take a sample of size b_sample_size with replacement
  train = houses[ind, ] #create your training dataset - this subset will always have b_sample_size observations
  test = houses[-ind, ] #create your out-of-bag or testing dataset - the size of this subset will vary depending on the number of repetitions in train
  mdl = lm(price ~ sqft, data = train) #train your model
  pred = predict(mdl, newdata = test) #try to predict using the out-of-bag samples
  RMSE_boot[b] = RMSE(pred, test$price) #calculate the relevant estimate you want to investigate
}
summary(RMSE_boot)
hist(RMSE_boot)
```

```{r}
n_sample = 2000 #choose the number of bootstrap samples you want to take
b_sample_size = 440 #set the sample size for each bootstrap sample
RMSE_boot = rep(NA, n_sample) #pre-allocate a vector to save the calculated RMSE values
for (b in 1:n_sample) {
  ind = sample(1:nrow(houses), size = b_sample_size, replace = TRUE) #take a sample of size b_sample_size with replacement
  train = houses[ind, ] #create your training dataset - this subset will always have b_sample_size observations
  test = houses[-ind, ] #create your out-of-bag or testing dataset - the size of this subset will vary depending on the number of repetitions in train
  mdl = lm(price ~ sqft, data = train) #train your model
  pred = predict(mdl, newdata = test) #try to predict using the out-of-bag samples
  RMSE_boot[b] = RMSE(pred, test$price) #calculate the relevant estimate you want to investigate
}
summary(RMSE_boot)
hist(RMSE_boot)
```

### Bagging Trees

Now let's use bagging on regression trees created using **rpart**. First we run a simple validation exercise to see what the output of a regression tree looks like:

```{r}
library(rpart)
set.seed(42)
ind = sample(1:nrow(houses), size = 442)
train_houses = houses[ind, ] # 442 samples for training
test_houses = houses[-ind, ] # 50 samples for validation
regpart = rpart(data = train_houses, price ~ sqft, xval = 0) #xval turns cross-validation off
pred_single = predict(regpart, newdata = test_houses) #use your regression tree to predict prices on the testing dataset
calc_RMSE = RMSE(pred_single, test_houses$price) #calculate the RMSE
noquote(c("The calculated RMSE is:", calc_RMSE)) #noquote is like print without the quotes
```

Now let's plot our predictions against the actual values in the testing dataset:

```{r}
pred_test = test_houses %>% mutate(pred_prices = pred_single)

library(ggplot2)

ggplot(pred_test) +
  geom_point(aes(x = price, y = pred_prices)) +
  geom_abline(intercept = 0,
              slope = 1,
              colour = "red")
```

A regression tree returns the same value for all points that fall in the same terminal node. This isn't a particularly good model but it is cheap and easy to fit and predict. This is how random forests start!

Before we add the random to random forests, let's use the Bagging process introduced earlier to bag a set of trees. We are going to create $n_{sample}$ trees trained with bootstrapped samples from a training dataset and save them to a list. We will use the out of bag samples to produce an estimate of the RMSE. We will then apply each model to the testing dataset and retrieve the predictions for each model and plot them:

```{r}
n_sample = 500
b_sample_size = 400
greta = list()
RMSE_boot = rep(NA,n_sample)
for (b in 1:n_sample){
  ind = sample(1:nrow(train_houses),size = b_sample_size, replace = TRUE)
  train = train_houses[ind,] #training sample, size 400
  test = train_houses[-ind,] #out-of-bag
  regpart = rpart(data = train, price ~ sqft, xval = 0)
  greta[[b]] = regpart #save the trees in a list
  pred = predict(regpart, newdata = test)
  RMSE_boot[b] = RMSE(pred, test$price)
}

print(mean(RMSE_boot))
```

Since we want to save all predicted values, we preallocate a matrix with $n_{sample}$ columns, We can then plot the average of the predicted price for each property in our test dataset:

```{r}
price_boot = matrix(NA, nrow = nrow(test_houses), ncol = n_sample)
RMSE_pred_boot = rep(NA, n_sample)
for (b in 1:n_sample){
  pred = predict(greta[[b]],newdata = test_houses)
  price_boot[,b] = pred
  RMSE_pred_boot[b] = RMSE(pred, test_houses$price)
}
mean_pred_prices = rowMeans(price_boot)
mean(RMSE_pred_boot)
```

```{r}
pred_test_bag = pred_test %>% mutate(pred_bag = mean_pred_prices)

ggplot(pred_test_bag) + geom_point(aes(x=price, y=pred_bag)) + geom_abline(intercept = 0,slope = 1, colour = "red")
```

We can see less repetition on the predicted values than before. Let's overlay the previous set of predictions using a single tree to compare:

```{r}
ggplot(pred_test_bag) +
  geom_point(aes(x=price, y=pred_bag)) + # bagged model
  geom_abline(intercept = 0,slope = 1, colour = "red") +
  geom_point(aes(x=price, y=pred_prices), colour = "blue") # First model
```

Let's take a look at the predicted values for each of the bootstrapped samples. Earlier we said that decision trees are high variance models. Here we will see that the training set can and does have a large impact on predicted values:

```{r}
# Matplot is easier and faster than ggplot for this task... not necessarily prettier
matplot(price_boot, pred_test_bag$price, col="grey", pch = 16)
points(pred_test_bag$pred_bag, pred_test_bag$price, col="black", pch = 16)
points(pred_test_bag$pred_prices, pred_test_bag$price, col = "blue", pch = 16)
abline(0,1, col="red")
```

**Exercise:** Repeat the process above, change the size of the training and testing datasets, and the bootstrap parameters. What happens when you reduce the size of your training dataset in the initial split?

#### Bagging with caret

Bagging can be paired with cross-validation. Ultimately each bagged model as we built them is a model built on a training set and validated on a testing set. We could have built a second loop that creates folds and implements cross-validation on the bagged trees. This is an exercise you should try; while caret makes training easier, it doesn't have the same flexibility as your own algorithm for selecting sample size during bootstrapping, nor can you easily access all the outputs we have generated above.

If you want to use **caret** to bag trees, you can use the procedure below:

```{r}
fitControl = trainControl(method = "cv", number = 10)
BagTrees = train(price ~ sqft,
                 data = train_houses,
                 method = "treebag",
                 nbagg = 500,
                 trControl = fitControl)
BagTrees #Take a break, this bit can take a minute or two. You are cross-validating and bootstrapping here!
```

```{r}
predCaretBags = test_houses %>% mutate(predBagCV = predict(BagTrees, newdata = test_houses))
ggplot(predCaretBags) + geom_point(aes(x=price, y=predBagCV)) + geom_abline(intercept = 0,slope = 1, colour = "red")

```

### Bag the trees, start a forest

So the random part of Random Forests doesn't come from the bootstrapped samples. It comes from the randomization of the features that are available to each tree.

Here is high-level description of how a Random Forest is created:

First you need a training dataset with $n_{train}$ observations and you need to choose the number of trees $n_{trees}$ you are going to grow in your forest.

```         
For each tree

1. take a bootstrap sample of training of size n_train
2. start growing a tree (at random!)

    for each split
    
        2.1. select k features at random from all p features
        2.2. pick the best split using the Gini Impurity
        2.3. split the node

3. stop growing based on stopping criteria (e.g. CP, entropy)

4. tree complete? Do not prune!

Return ensemble of trees. Congratulations you have grown a random forest!
```

If k = p, you have bagging. Note that in step 1, the bootstrap sample has the same size as the training dataset. This is the format of the classic bootstrap as developed in the (19)70s. Random Forests were created in 1995, before computers were powerful enough to do what you are doing here today. Current implementations of random forests may use different sample sizes.

We are going to use the **randomForest** library to fit our forests. The main **randomForest** function has many parameters, most of which you don't need to change as they are likely to have a very small impact on your final model.

The main model parameters are:

-   **number of trees** in the forest
-   **number of features** to be considered in each split
-   **tree complexity** - leave it alone, it's unlikely you'll need to change this
-   **sampling scheme** - leave it alone, bootstrap is fine except if you have lots of categorical variables then you should consider sampling without replacement
-   **splitting rule** - leave it alone, it's unlikely a different rule will make your model better

```{r}
library(randomForest)
```

```{r}
set.seed(42)
ind = sample(1:nrow(houses), size = 50)
test_houses = houses[ind,]
train_houses = houses[-ind, ]
```

```{r}
rf = randomForest(price ~ sqft, data = train_houses)
rf
```

```{r}
predRF = test_houses %>% mutate(pred_rf = predict(rf, newdata = test_houses))
RMSE(predRF$pred_rf, predRF$price)
ggplot(predRF) + geom_point(aes(x=price, y=pred_rf)) + geom_abline(intercept = 0,slope = 1, colour = "red")
```

This is it. You have built a random forest. Now go back and add more features to all your models. Here is what we get when we include all features in our forest. For this one, we have chosen the number of features in $k$ to be 5. Try changing this parameter as well as the number of trees, how well can you tune your forest? Check both the variance explained in the model output and the RMSE against the testing dataset.

```{r}
rf = randomForest(price ~ ., data = train_houses, mtry = 5)
rf
```

```{r}
predRF = test_houses %>% mutate(pred_rf = predict(rf, newdata = test_houses))
RMSE(predRF$pred_rf, predRF$price)
ggplot(predRF) + geom_point(aes(x=price, y=pred_rf)) + geom_abline(intercept = 0,slope = 1, colour = "red")
```

And here is how you can use the same function to build a random forest for classification. You should monitor your model out-of-bag error rate and the confusion matrix as you change the number of variables, trees, etc.

```{r}
rf = randomForest(factor(city) ~ sqft, data = train_houses)
rf
```

```{r}
pred = predict(rf, newdata = test_houses)
confusionMatrix(pred, factor(test_houses$city))
```

```{r}
rf = randomForest(factor(city) ~ ., data = train_houses)
rf
```

```{r}
pred = predict(rf, newdata = test_houses)
confusionMatrix(pred, factor(test_houses$city))
```

**Exercise:** Up to this point, we have asked to try to tune your random forests by hand. Now try to build a tuning procedure using **randomForest** and **caret**. You might need to do some research on how those two packages can work together. You should also try to add cross-validation to your tuned model.

```{r}

```
